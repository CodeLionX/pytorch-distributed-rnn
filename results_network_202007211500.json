{"results": [{"trainer": "distributed", "rule_type": "delay", "rule_value": 0, "rule": "delay 0ms", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:3: Memory Usage: 191.90234375, Training Duration: 27.548037182999906\nINFO:root:4: Memory Usage: 192.6171875, Training Duration: 27.54891659199984\nINFO:root:7: Memory Usage: 193.18359375, Training Duration: 27.562760327999968\nINFO:root:10: Memory Usage: 192.50390625, Training Duration: 27.567212586000096\nINFO:root:5: Memory Usage: 192.453125, Training Duration: 27.565967995999927\nINFO:root:6: Memory Usage: 192.6328125, Training Duration: 27.56984050500023\nINFO:root:11: Memory Usage: 191.5546875, Training Duration: 27.565400989999944\nINFO:root:2: Memory Usage: 193.26953125, Training Duration: 27.565461976999813\nINFO:root:8: Memory Usage: 192.4921875, Training Duration: 27.565306924000197\nINFO:root:1: Memory Usage: 192.94140625, Training Duration: 27.566189290999773\nINFO:root:9: Memory Usage: 192.26953125, Training Duration: 27.566378411000187\nINFO:root:0: Memory Usage: 192.01953125, Training Duration: 27.57422988899998\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "delay", "rule_value": 0, "rule": "delay 0ms", "stdout": "", "stderr": "[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792811\tAcc: 17/120 (14%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.6328125, Training Duration: 72.45774526900004\n[1,9]<stderr>:INFO:root:9: Memory Usage: 193.2734375, Training Duration: 72.4507974510002\n[1,3]<stderr>:INFO:root:3: Memory Usage: 193.41796875, Training Duration: 72.45282030600038\n[1,4]<stderr>:INFO:root:4: Memory Usage: 193.80078125, Training Duration: 72.45254990600006\n[1,2]<stderr>:INFO:root:2: Memory Usage: 195.08203125, Training Duration: 72.45402508000006\n[1,11]<stderr>:INFO:root:11: Memory Usage: 192.13671875, Training Duration: 72.45482073499988\n[1,10]<stderr>:INFO:root:10: Memory Usage: 193.76171875, Training Duration: 72.45479468899975\n[1,1]<stderr>:INFO:root:1: Memory Usage: 191.3828125, Training Duration: 72.45689866200019\n[1,6]<stderr>:INFO:root:6: Memory Usage: 193.66015625, Training Duration: 72.45923757499986\n[1,5]<stderr>:INFO:root:5: Memory Usage: 191.69921875, Training Duration: 72.46108752400005\n[1,7]<stderr>:INFO:root:7: Memory Usage: 192.55078125, Training Duration: 72.46271698700002\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.9609375, Training Duration: 72.47273998500032\n", "command": "~/susml/jakob_torben/bin/horovodrun -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "delay", "rule_value": 1, "rule": "delay 1ms", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:3: Memory Usage: 193.69140625, Training Duration: 25.742029683000055\nINFO:root:2: Memory Usage: 191.7109375, Training Duration: 25.744271531999857\nINFO:root:6: Memory Usage: 192.49609375, Training Duration: 25.737822929999766\nINFO:root:5: Memory Usage: 192.4453125, Training Duration: 25.733790337999835\nINFO:root:8: Memory Usage: 193.29296875, Training Duration: 25.7398389330001\nINFO:root:0: Memory Usage: 192.515625, Training Duration: 25.759494777999862\nINFO:root:7: Memory Usage: 193.80859375, Training Duration: 25.739919293000185\nINFO:root:4: Memory Usage: 192.76953125, Training Duration: 25.747261610999885\nINFO:root:9: Memory Usage: 193.01171875, Training Duration: 25.73064067400037\nINFO:root:11: Memory Usage: 191.3046875, Training Duration: 25.754047074000027\nINFO:root:10: Memory Usage: 192.109375, Training Duration: 25.73650369799998\nINFO:root:1: Memory Usage: 191.43359375, Training Duration: 25.747250436000286\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "delay", "rule_value": 1, "rule": "delay 1ms", "stdout": "", "stderr": "[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,11]<stderr>:INFO:root:11: Memory Usage: 192.203125, Training Duration: 61.15864235800018\n[1,10]<stderr>:INFO:root:10: Memory Usage: 194.6015625, Training Duration: 61.15667737300009\n[1,9]<stderr>:INFO:root:9: Memory Usage: 192.8984375, Training Duration: 61.15696455299985\n[1,1]<stderr>:INFO:root:1: Memory Usage: 192.26171875, Training Duration: 61.18044260199986\n[1,6]<stderr>:INFO:root:6: Memory Usage: 192.6796875, Training Duration: 61.18374676299982\n[1,7]<stderr>:INFO:root:7: Memory Usage: 191.41015625, Training Duration: 61.17730257899984\n[1,4]<stderr>:INFO:root:4: Memory Usage: 192.65234375, Training Duration: 61.180433021000226\n[1,0]<stderr>:INFO:root:0: Memory Usage: 191.546875, Training Duration: 61.193225256999995\n[1,8]<stderr>:INFO:root:8: Memory Usage: 191.6640625, Training Duration: 61.18097568100029\n[1,2]<stderr>:INFO:root:2: Memory Usage: 194.39453125, Training Duration: 61.1840405199996\n[1,3]<stderr>:INFO:root:3: Memory Usage: 192.359375, Training Duration: 61.18436703899988\n[1,5]<stderr>:INFO:root:5: Memory Usage: 194.11328125, Training Duration: 61.18603390099997\n", "command": "~/susml/jakob_torben/bin/horovodrun -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "delay", "rule_value": 2, "rule": "delay 2ms", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:5: Memory Usage: 193.83203125, Training Duration: 25.64014736199988\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:9: Memory Usage: 191.4296875, Training Duration: 25.633840319999763\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:3: Memory Usage: 194.0859375, Training Duration: 25.65133820400024\nINFO:root:4: Memory Usage: 191.06640625, Training Duration: 25.653066658999705\nINFO:root:6: Memory Usage: 191.6484375, Training Duration: 25.6670663230002\nINFO:root:8: Memory Usage: 193.00390625, Training Duration: 25.65948309899977\nINFO:root:7: Memory Usage: 195.109375, Training Duration: 25.664283178999995\nINFO:root:0: Memory Usage: 192.12109375, Training Duration: 25.696815618000073\nINFO:root:10: Memory Usage: 192.41796875, Training Duration: 25.660578002999955\nINFO:root:1: Memory Usage: 192.97265625, Training Duration: 25.679092480000236\nINFO:root:2: Memory Usage: 193.140625, Training Duration: 25.685150003000217\nINFO:root:11: Memory Usage: 193.7890625, Training Duration: 25.686774661999607\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "delay", "rule_value": 2, "rule": "delay 2ms", "stdout": "", "stderr": "[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,7]<stderr>:INFO:root:7: Memory Usage: 192.33984375, Training Duration: 74.7595220090002\n[1,4]<stderr>:INFO:root:4: Memory Usage: 191.6875, Training Duration: 74.76375604700024\n[1,3]<stderr>:INFO:root:3: Memory Usage: 191.6015625, Training Duration: 74.76363885\n[1,6]<stderr>:INFO:root:6: Memory Usage: 192.8671875, Training Duration: 74.7540452469998\n[1,2]<stderr>:INFO:root:2: Memory Usage: 192.92578125, Training Duration: 74.7643199729996\n[1,11]<stderr>:INFO:root:11: Memory Usage: 193.87890625, Training Duration: 74.76498304300003\n[1,10]<stderr>:INFO:root:10: Memory Usage: 193.90234375, Training Duration: 74.76349301799974\n[1,9]<stderr>:INFO:root:9: Memory Usage: 192.90234375, Training Duration: 74.7630508950001\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.65234375, Training Duration: 74.76481671700003\n[1,1]<stderr>:INFO:root:1: Memory Usage: 194.7109375, Training Duration: 74.77060031899964\n[1,0]<stderr>:INFO:root:0: Memory Usage: 193.6015625, Training Duration: 74.79874814599998\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.984375, Training Duration: 74.78038904699997\n", "command": "~/susml/jakob_torben/bin/horovodrun -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "delay", "rule_value": 5, "rule": "delay 5ms", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:11: Memory Usage: 190.859375, Training Duration: 27.86663981899983\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:3: Memory Usage: 194.13671875, Training Duration: 27.861848147999808\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:4: Memory Usage: 192.58984375, Training Duration: 27.861276037999687\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:5: Memory Usage: 191.734375, Training Duration: 27.86182113399991\nINFO:root:0: Memory Usage: 193.5625, Training Duration: 27.94220841000015\nINFO:root:10: Memory Usage: 194.23828125, Training Duration: 27.85714200500024\nINFO:root:2: Memory Usage: 194.55078125, Training Duration: 27.894836942999973\nINFO:root:1: Memory Usage: 191.72265625, Training Duration: 27.898765356999775\nINFO:root:6: Memory Usage: 192.4140625, Training Duration: 27.870952474000205\nINFO:root:7: Memory Usage: 193.53125, Training Duration: 27.87263955299977\nINFO:root:9: Memory Usage: 192.59375, Training Duration: 27.872007526999823\nINFO:root:8: Memory Usage: 193.00390625, Training Duration: 27.87364849000005\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "delay", "rule_value": 5, "rule": "delay 5ms", "stdout": "", "stderr": "[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.8671875, Training Duration: 89.87632789700001\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,3]<stderr>:INFO:root:3: Memory Usage: 193.078125, Training Duration: 89.87868790200037\n[1,10]<stderr>:INFO:root:10: Memory Usage: 193.38671875, Training Duration: 89.8750969130001\n[1,7]<stderr>:INFO:root:7: Memory Usage: 193.6015625, Training Duration: 89.89295392900021\n[1,11]<stderr>:INFO:root:11: Memory Usage: 192.05078125, Training Duration: 89.9034891870001\n[1,6]<stderr>:INFO:root:6: Memory Usage: 192.72265625, Training Duration: 89.89357353600008\n[1,4]<stderr>:INFO:root:4: Memory Usage: 193.4453125, Training Duration: 89.90366695600005\n[1,1]<stderr>:INFO:root:1: Memory Usage: 190.72265625, Training Duration: 89.91002940399994\n[1,9]<stderr>:INFO:root:9: Memory Usage: 191.265625, Training Duration: 89.89914117600028\n[1,2]<stderr>:INFO:root:2: Memory Usage: 192.140625, Training Duration: 89.90915613700008\n[1,8]<stderr>:INFO:root:8: Memory Usage: 191.4296875, Training Duration: 89.90580809100038\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.39453125, Training Duration: 89.96140231000027\n", "command": "~/susml/jakob_torben/bin/horovodrun -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "delay", "rule_value": 10, "rule": "delay 10ms", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:11: Memory Usage: 192.578125, Training Duration: 28.07984848699971\nINFO:root:3: Memory Usage: 192.12890625, Training Duration: 28.050044766000156\nINFO:root:5: Memory Usage: 191.74609375, Training Duration: 28.04128190099982\nINFO:root:0: Memory Usage: 193.9453125, Training Duration: 28.182000934999905\nINFO:root:1: Memory Usage: 194.27734375, Training Duration: 28.089382315999956\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:4: Memory Usage: 192.66796875, Training Duration: 28.050772553000115\nINFO:root:2: Memory Usage: 191.890625, Training Duration: 28.096950800000286\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:6: Memory Usage: 191.734375, Training Duration: 28.067090517999986\nINFO:root:10: Memory Usage: 193.71875, Training Duration: 28.040032311999767\nINFO:root:7: Memory Usage: 193.04296875, Training Duration: 28.057298834999983\nINFO:root:9: Memory Usage: 191.8828125, Training Duration: 28.05409241999996\nINFO:root:8: Memory Usage: 193.7734375, Training Duration: 28.055904878000092\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "delay", "rule_value": 10, "rule": "delay 10ms", "stdout": "", "stderr": "[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.91796875, Training Duration: 87.30527666199987\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,3]<stderr>:INFO:root:3: Memory Usage: 192.23046875, Training Duration: 87.30604312900005\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,4]<stderr>:INFO:root:4: Memory Usage: 190.83203125, Training Duration: 87.31602850200034\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,1]<stderr>:INFO:root:1: Memory Usage: 193.59375, Training Duration: 87.33044659500001\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.03125, Training Duration: 87.42245773000013\n[1,6]<stderr>:INFO:root:6: Memory Usage: 191.69921875, Training Duration: 87.32532019000018\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.34375, Training Duration: 87.32006029700005\n[1,2]<stderr>:INFO:root:2: Memory Usage: 194.328125, Training Duration: 87.34734025499984\n[1,10]<stderr>:INFO:root:10: Memory Usage: 191.73828125, Training Duration: 87.33084511600009\n[1,9]<stderr>:INFO:root:9: Memory Usage: 191.37890625, Training Duration: 87.33004583299999\n[1,7]<stderr>:INFO:root:7: Memory Usage: 193.5625, Training Duration: 87.32879414299987\n[1,11]<stderr>:INFO:root:11: Memory Usage: 193.390625, Training Duration: 87.35156935799978\n", "command": "~/susml/jakob_torben/bin/horovodrun -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "delay", "rule_value": 25, "rule": "delay 25ms", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:0: Memory Usage: 193.984375, Training Duration: 28.36577616800014\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:11: Memory Usage: 194.2265625, Training Duration: 28.116780291000396\nINFO:root:1: Memory Usage: 192.13671875, Training Duration: 28.141480277000028\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:2: Memory Usage: 190.3828125, Training Duration: 28.169197222000093\nINFO:root:5: Memory Usage: 192.921875, Training Duration: 28.046918201999688\nINFO:root:3: Memory Usage: 193.00390625, Training Duration: 28.073766287000126\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:4: Memory Usage: 193.21484375, Training Duration: 28.07418159300005\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:6: Memory Usage: 192.36328125, Training Duration: 28.09415933399987\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:7: Memory Usage: 192.796875, Training Duration: 28.065404631000092\nINFO:root:9: Memory Usage: 191.82421875, Training Duration: 28.041528973999903\nINFO:root:10: Memory Usage: 192.2421875, Training Duration: 28.053960312000072\nINFO:root:8: Memory Usage: 191.69921875, Training Duration: 28.06736208600023\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "delay", "rule_value": 25, "rule": "delay 25ms", "stdout": "", "stderr": "[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,1]<stderr>:INFO:root:1: Memory Usage: 192.1015625, Training Duration: 91.02622870000005\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.41015625, Training Duration: 91.25488393600017\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,3]<stderr>:INFO:root:3: Memory Usage: 192.36328125, Training Duration: 91.00935367800002\n[1,9]<stderr>:INFO:root:9: Memory Usage: 192.94921875, Training Duration: 90.98480383300011\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.64453125, Training Duration: 91.01290286099993\n[1,7]<stderr>:INFO:root:7: Memory Usage: 193.5234375, Training Duration: 90.99069043300005\n[1,8]<stderr>:INFO:root:8: Memory Usage: 191.51171875, Training Duration: 91.00404499099977\n[1,2]<stderr>:INFO:root:2: Memory Usage: 194.83203125, Training Duration: 91.05991700899995\n[1,6]<stderr>:INFO:root:6: Memory Usage: 191.4375, Training Duration: 91.03901767300022\n[1,4]<stderr>:INFO:root:4: Memory Usage: 191.7421875, Training Duration: 91.03805631299974\n[1,10]<stderr>:INFO:root:10: Memory Usage: 192.453125, Training Duration: 91.01378939699998\n[1,11]<stderr>:INFO:root:11: Memory Usage: 192.5625, Training Duration: 91.04486178699972\n", "command": "~/susml/jakob_torben/bin/horovodrun -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "delay", "rule_value": 50, "rule": "delay 50ms", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:9: Memory Usage: 192.3515625, Training Duration: 28.436173754000265\nINFO:root:0: Memory Usage: 191.51171875, Training Duration: 29.298948043999644\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:10: Memory Usage: 192.703125, Training Duration: 28.480274414999712\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:11: Memory Usage: 192.4453125, Training Duration: 28.88437975199986\nINFO:root:1: Memory Usage: 192.48046875, Training Duration: 28.94788698000002\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:4: Memory Usage: 194.2265625, Training Duration: 28.733877697000025\nINFO:root:2: Memory Usage: 191.86328125, Training Duration: 29.000220003999857\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:3: Memory Usage: 191.07421875, Training Duration: 28.83321172099977\nINFO:root:5: Memory Usage: 193.9453125, Training Duration: 28.77834829199992\nINFO:root:7: Memory Usage: 193.60546875, Training Duration: 28.702441378000003\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:6: Memory Usage: 191.265625, Training Duration: 28.853137414999765\nINFO:root:8: Memory Usage: 192.92578125, Training Duration: 28.70159777200024\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "delay", "rule_value": 50, "rule": "delay 50ms", "stdout": "", "stderr": "[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,3]<stderr>:INFO:root:3: Memory Usage: 191.65234375, Training Duration: 102.86760395400006\n[1,5]<stderr>:INFO:root:5: Memory Usage: 194.046875, Training Duration: 102.86960554200004\n[1,7]<stderr>:INFO:root:7: Memory Usage: 194.44921875, Training Duration: 102.84143446600001\n[1,1]<stderr>:INFO:root:1: Memory Usage: 191.41015625, Training Duration: 102.94798631699996\n[1,0]<stderr>:INFO:root:0: Memory Usage: 193.25390625, Training Duration: 103.40134149699998\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,9]<stderr>:INFO:root:9: Memory Usage: 192.875, Training Duration: 102.86544596699969\n[1,11]<stderr>:INFO:root:11: Memory Usage: 192.8828125, Training Duration: 102.92154458200002\n[1,10]<stderr>:INFO:root:10: Memory Usage: 194.3125, Training Duration: 102.871296282\n[1,6]<stderr>:INFO:root:6: Memory Usage: 191.84765625, Training Duration: 102.92340568600002\n[1,2]<stderr>:INFO:root:2: Memory Usage: 193.65625, Training Duration: 102.99121774800005\n[1,4]<stderr>:INFO:root:4: Memory Usage: 192.18359375, Training Duration: 102.94628126900034\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.3046875, Training Duration: 102.89757387500003\n", "command": "~/susml/jakob_torben/bin/horovodrun -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "delay", "rule_value": 100, "rule": "delay 100ms", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:5: Memory Usage: 193.7734375, Training Duration: 34.15234265700019\nINFO:root:7: Memory Usage: 193.6875, Training Duration: 33.973469278000266\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:6: Memory Usage: 192.40625, Training Duration: 34.275026115999935\nINFO:root:8: Memory Usage: 191.94140625, Training Duration: 33.97525174900011\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:9: Memory Usage: 191.921875, Training Duration: 34.05347569700007\nINFO:root:0: Memory Usage: 192.32421875, Training Duration: 35.77702280500034\nINFO:root:11: Memory Usage: 193.7734375, Training Duration: 34.78386306699986\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:10: Memory Usage: 192.90234375, Training Duration: 34.15498797600003\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:3: Memory Usage: 192.03125, Training Duration: 34.66034798199962\nINFO:root:1: Memory Usage: 192.93359375, Training Duration: 35.073861577000116\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:4: Memory Usage: 193.125, Training Duration: 34.660335857000064\nINFO:root:2: Memory Usage: 193.19140625, Training Duration: 35.17634537899994\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "delay", "rule_value": 100, "rule": "delay 100ms", "stdout": "", "stderr": "[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792811\tAcc: 17/120 (14%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,5]<stderr>:INFO:root:5: Memory Usage: 193.09375, Training Duration: 107.21838929599971\n[1,3]<stderr>:INFO:root:3: Memory Usage: 192.60546875, Training Duration: 107.22341767999978\n[1,6]<stderr>:INFO:root:6: Memory Usage: 192.86328125, Training Duration: 107.22784329200022\n[1,10]<stderr>:INFO:root:10: Memory Usage: 194.44921875, Training Duration: 107.12543903400001\n[1,9]<stderr>:INFO:root:9: Memory Usage: 192.05078125, Training Duration: 107.12389696800028\n[1,11]<stderr>:INFO:root:11: Memory Usage: 192.84765625, Training Duration: 107.23719610999979\n[1,0]<stderr>:INFO:root:0: Memory Usage: 193.9609375, Training Duration: 108.24768310400032\n[1,1]<stderr>:INFO:root:1: Memory Usage: 191.125, Training Duration: 107.35115345600025\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,4]<stderr>:INFO:root:4: Memory Usage: 193.87109375, Training Duration: 107.31662613700018\n[1,7]<stderr>:INFO:root:7: Memory Usage: 193.0234375, Training Duration: 107.24167664800007\n[1,8]<stderr>:INFO:root:8: Memory Usage: 193.0078125, Training Duration: 107.2488535790003\n[1,2]<stderr>:INFO:root:2: Memory Usage: 194.62890625, Training Duration: 107.45089871200025\n", "command": "~/susml/jakob_torben/bin/horovodrun -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "loss", "rule_value": 0, "rule": "loss 0%", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:7: Memory Usage: 195.015625, Training Duration: 23.628790091000155\nINFO:root:9: Memory Usage: 192.68359375, Training Duration: 23.624764475999655\nINFO:root:6: Memory Usage: 192.09765625, Training Duration: 23.626283643999614\nINFO:root:3: Memory Usage: 193.125, Training Duration: 23.63303268799973\nINFO:root:0: Memory Usage: 193.8515625, Training Duration: 23.637963088000106\nINFO:root:8: Memory Usage: 190.86328125, Training Duration: 23.630127937000452\nINFO:root:4: Memory Usage: 192.89453125, Training Duration: 23.630082296000182\nINFO:root:5: Memory Usage: 192.3671875, Training Duration: 23.629523100999904\nINFO:root:1: Memory Usage: 190.98828125, Training Duration: 23.63217613500001\nINFO:root:10: Memory Usage: 193.76953125, Training Duration: 23.627718626999922\nINFO:root:2: Memory Usage: 193.0546875, Training Duration: 23.633270092000203\nINFO:root:11: Memory Usage: 194.27734375, Training Duration: 23.639395109999896\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "loss", "rule_value": 0, "rule": "loss 0%", "stdout": "", "stderr": "[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,6]<stderr>:INFO:root:6: Memory Usage: 193.44921875, Training Duration: 50.45836921499995\n[1,1]<stderr>:INFO:root:1: Memory Usage: 194.73828125, Training Duration: 50.458684577999975\n[1,9]<stderr>:INFO:root:9: Memory Usage: 192.49609375, Training Duration: 50.45789944600028\n[1,4]<stderr>:INFO:root:4: Memory Usage: 190.515625, Training Duration: 50.457671738000045\n[1,10]<stderr>:INFO:root:10: Memory Usage: 192.3515625, Training Duration: 50.458354021000105\n[1,11]<stderr>:INFO:root:11: Memory Usage: 191.3515625, Training Duration: 50.45963436000011\n[1,3]<stderr>:INFO:root:3: Memory Usage: 192.6484375, Training Duration: 50.459657921000144\n[1,7]<stderr>:INFO:root:7: Memory Usage: 192.96484375, Training Duration: 50.4589461569999\n[1,2]<stderr>:INFO:root:2: Memory Usage: 193.171875, Training Duration: 50.45947089299989\n[1,0]<stderr>:INFO:root:0: Memory Usage: 193.734375, Training Duration: 50.46340877600005\n[1,5]<stderr>:INFO:root:5: Memory Usage: 193.9140625, Training Duration: 50.46152555500021\n[1,8]<stderr>:INFO:root:8: Memory Usage: 193.84765625, Training Duration: 50.46207721100018\n", "command": "~/susml/jakob_torben/bin/horovodrun -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "loss", "rule_value": 0.1, "rule": "loss 0.1%", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:10: Memory Usage: 193.84375, Training Duration: 24.267423427999802\nINFO:root:4: Memory Usage: 193.18359375, Training Duration: 24.270755053999892\nINFO:root:3: Memory Usage: 192.70703125, Training Duration: 24.27035337799998\nINFO:root:5: Memory Usage: 191.98828125, Training Duration: 24.266930152999976\nINFO:root:0: Memory Usage: 192.25, Training Duration: 24.29600677999997\nINFO:root:6: Memory Usage: 191.70703125, Training Duration: 24.287066534000132\nINFO:root:7: Memory Usage: 193.5, Training Duration: 24.28880598600017\nINFO:root:2: Memory Usage: 191.49609375, Training Duration: 24.28989360100013\nINFO:root:11: Memory Usage: 192.2421875, Training Duration: 24.298604290000185\nINFO:root:8: Memory Usage: 193.02734375, Training Duration: 24.260304961999736\nINFO:root:9: Memory Usage: 192.58984375, Training Duration: 24.287477752000086\nINFO:root:1: Memory Usage: 191.70703125, Training Duration: 24.29512578899994\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "loss", "rule_value": 0.1, "rule": "loss 0.1%", "stdout": "", "stderr": "[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.7578125, Training Duration: 52.38516063899988\n[1,1]<stderr>:INFO:root:1: Memory Usage: 194.078125, Training Duration: 52.386190755999905\n[1,11]<stderr>:INFO:root:11: Memory Usage: 192.91796875, Training Duration: 52.387171255999874\n[1,4]<stderr>:INFO:root:4: Memory Usage: 192.265625, Training Duration: 52.38698971599979\n[1,10]<stderr>:INFO:root:10: Memory Usage: 193.3125, Training Duration: 52.38627970799962\n[1,2]<stderr>:INFO:root:2: Memory Usage: 191.8203125, Training Duration: 52.382727886999874\n[1,5]<stderr>:INFO:root:5: Memory Usage: 193.46484375, Training Duration: 52.386934215999645\n[1,0]<stderr>:INFO:root:0: Memory Usage: 191.2109375, Training Duration: 52.390821396999854\n[1,6]<stderr>:INFO:root:6: Memory Usage: 193.046875, Training Duration: 52.38997864199973\n[1,3]<stderr>:INFO:root:3: Memory Usage: 192.6015625, Training Duration: 52.38774923300025\n[1,7]<stderr>:INFO:root:7: Memory Usage: 190.6328125, Training Duration: 52.38905551800008\n[1,9]<stderr>:INFO:root:9: Memory Usage: 193.0546875, Training Duration: 52.38859058499975\n", "command": "~/susml/jakob_torben/bin/horovodrun -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "loss", "rule_value": 0.5, "rule": "loss 0.5%", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:3: Memory Usage: 194.0078125, Training Duration: 27.526018745999863\nINFO:root:4: Memory Usage: 191.35546875, Training Duration: 27.52692004399978\nINFO:root:10: Memory Usage: 191.9140625, Training Duration: 27.52657134600031\nINFO:root:5: Memory Usage: 192.39453125, Training Duration: 27.526474270999643\nINFO:root:6: Memory Usage: 191.98828125, Training Duration: 27.547720800000207\nINFO:root:9: Memory Usage: 193.94921875, Training Duration: 27.54568140900028\nINFO:root:2: Memory Usage: 191.69921875, Training Duration: 27.540810431999944\nINFO:root:1: Memory Usage: 194.328125, Training Duration: 27.54320716400025\nINFO:root:7: Memory Usage: 193.80078125, Training Duration: 27.54509307499984\nINFO:root:8: Memory Usage: 192.1171875, Training Duration: 27.54474017700022\nINFO:root:11: Memory Usage: 192.84765625, Training Duration: 27.546251226000095\nINFO:root:0: Memory Usage: 191.8359375, Training Duration: 27.559950882000066\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "loss", "rule_value": 0.5, "rule": "loss 0.5%", "stdout": "", "stderr": "[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,10]<stderr>:INFO:root:10: Memory Usage: 194.09765625, Training Duration: 61.296841392999795\n[1,0]<stderr>:INFO:root:0: Memory Usage: 191.12890625, Training Duration: 61.30087691300014\n[1,8]<stderr>:INFO:root:8: Memory Usage: 193.14453125, Training Duration: 61.297523016000014\n[1,3]<stderr>:INFO:root:3: Memory Usage: 194.43359375, Training Duration: 61.297839910999755\n[1,4]<stderr>:INFO:root:4: Memory Usage: 199.203125, Training Duration: 61.29811314000017\n[1,5]<stderr>:INFO:root:5: Memory Usage: 193.58203125, Training Duration: 61.29931264100014\n[1,2]<stderr>:INFO:root:2: Memory Usage: 193.91015625, Training Duration: 61.2955961560001\n[1,9]<stderr>:INFO:root:9: Memory Usage: 192.703125, Training Duration: 61.2980833810002\n[1,1]<stderr>:INFO:root:1: Memory Usage: 192.85546875, Training Duration: 61.300322928999776\n[1,6]<stderr>:INFO:root:6: Memory Usage: 194.13671875, Training Duration: 61.30238254200003\n[1,7]<stderr>:INFO:root:7: Memory Usage: 193.75, Training Duration: 61.29851280899993\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,11]<stderr>:INFO:root:11: Memory Usage: 192.3203125, Training Duration: 61.30175189300007\n", "command": "~/susml/jakob_torben/bin/horovodrun -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "loss", "rule_value": 1, "rule": "loss 1%", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:7: Memory Usage: 193.60546875, Training Duration: 22.91701882400048\nINFO:root:3: Memory Usage: 193.00390625, Training Duration: 22.92030564400011\nINFO:root:4: Memory Usage: 190.75, Training Duration: 22.921540825999728\nINFO:root:10: Memory Usage: 192.65625, Training Duration: 22.921946841000135\nINFO:root:6: Memory Usage: 192.8671875, Training Duration: 22.92253864600025\nINFO:root:0: Memory Usage: 192.140625, Training Duration: 21.908659112999885\nINFO:root:9: Memory Usage: 190.4765625, Training Duration: 22.924167368999406\nINFO:root:1: Memory Usage: 193.19921875, Training Duration: 22.918211977999817\nINFO:root:2: Memory Usage: 192.125, Training Duration: 22.92128958400008\nINFO:root:8: Memory Usage: 191.6875, Training Duration: 22.91921320899928\nINFO:root:5: Memory Usage: 193.390625, Training Duration: 22.9252073560001\nINFO:root:11: Memory Usage: 192.3515625, Training Duration: 21.909819157999664\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "loss", "rule_value": 1, "rule": "loss 1%", "stdout": "", "stderr": "[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,7]<stderr>:INFO:root:7: Memory Usage: 192.2109375, Training Duration: 71.2555717639998\n[1,9]<stderr>:INFO:root:9: Memory Usage: 193.77734375, Training Duration: 71.25627412100039\n[1,11]<stderr>:INFO:root:11: Memory Usage: 192.8828125, Training Duration: 71.24916142300026\n[1,6]<stderr>:INFO:root:6: Memory Usage: 194.0625, Training Duration: 71.26720241800012\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,3]<stderr>:INFO:root:3: Memory Usage: 191.9765625, Training Duration: 71.27707175500018\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,4]<stderr>:INFO:root:4: Memory Usage: 192.12109375, Training Duration: 71.28984799599948\n[1,2]<stderr>:INFO:root:2: Memory Usage: 191.8125, Training Duration: 71.31120160200044\n[1,10]<stderr>:INFO:root:10: Memory Usage: 192.84765625, Training Duration: 71.31172359100037\n[1,8]<stderr>:INFO:root:8: Memory Usage: 191.23828125, Training Duration: 71.30743439699927\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.25, Training Duration: 71.32011009600046\n[1,1]<stderr>:INFO:root:1: Memory Usage: 195.21484375, Training Duration: 71.30813598900022\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.453125, Training Duration: 71.31254940400049\n", "command": "~/susml/jakob_torben/bin/horovodrun -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "loss", "rule_value": 2, "rule": "loss 2%", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:10: Memory Usage: 192.85546875, Training Duration: 27.335204048000378\nINFO:root:0: Memory Usage: 192.25, Training Duration: 27.358638529000018\nINFO:root:3: Memory Usage: 192.9140625, Training Duration: 26.35722752899983\nINFO:root:7: Memory Usage: 193.05859375, Training Duration: 26.35555614199984\nINFO:root:5: Memory Usage: 192.23828125, Training Duration: 27.354874320999443\nINFO:root:8: Memory Usage: 193.046875, Training Duration: 26.351936977999685\nINFO:root:1: Memory Usage: 191.71484375, Training Duration: 27.316716540000016\nINFO:root:6: Memory Usage: 192.54296875, Training Duration: 27.36242211300032\nINFO:root:11: Memory Usage: 192.00390625, Training Duration: 26.353008633999707\nINFO:root:4: Memory Usage: 190.70703125, Training Duration: 26.35914384300031\nINFO:root:9: Memory Usage: 191.34375, Training Duration: 27.358522857000025\nINFO:root:2: Memory Usage: 193.51953125, Training Duration: 27.357101635000618\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "loss", "rule_value": 2, "rule": "loss 2%", "stdout": "", "stderr": "[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,4]<stderr>:INFO:root:4: Memory Usage: 192.20703125, Training Duration: 77.01493666799979\n[1,3]<stderr>:INFO:root:3: Memory Usage: 191.6015625, Training Duration: 77.0152474079996\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.26171875, Training Duration: 77.01276193000012\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.7578125, Training Duration: 77.04129813599957\n[1,7]<stderr>:INFO:root:7: Memory Usage: 192.421875, Training Duration: 77.04234105700016\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.1796875, Training Duration: 77.03545364399997\n[1,9]<stderr>:INFO:root:9: Memory Usage: 193.23828125, Training Duration: 77.01374109700009\n[1,6]<stderr>:INFO:root:6: Memory Usage: 193.1328125, Training Duration: 77.05032354600007\n[1,1]<stderr>:INFO:root:1: Memory Usage: 191.984375, Training Duration: 77.03444815200055\n[1,11]<stderr>:INFO:root:11: Memory Usage: 193.109375, Training Duration: 77.03849342299964\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,10]<stderr>:INFO:root:10: Memory Usage: 192.2578125, Training Duration: 77.05933603099948\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,2]<stderr>:INFO:root:2: Memory Usage: 192.24609375, Training Duration: 77.13176883300002\n", "command": "~/susml/jakob_torben/bin/horovodrun -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "loss", "rule_value": 5, "rule": "loss 5%", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:5: Memory Usage: 191.71875, Training Duration: 24.268220294999992\nINFO:root:6: Memory Usage: 192.5625, Training Duration: 24.26221931300006\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:8: Memory Usage: 193.18359375, Training Duration: 23.710207740999977\nINFO:root:4: Memory Usage: 194.01953125, Training Duration: 23.71737431300062\nINFO:root:7: Memory Usage: 192.85546875, Training Duration: 23.718463387000156\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:10: Memory Usage: 191.99609375, Training Duration: 24.559547613000177\nINFO:root:9: Memory Usage: 192.21875, Training Duration: 24.56254344900026\nINFO:root:2: Memory Usage: 192.19921875, Training Duration: 24.5611895359998\nINFO:root:3: Memory Usage: 193.2421875, Training Duration: 23.75297127199974\nINFO:root:1: Memory Usage: 193.31640625, Training Duration: 24.558073885999875\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:11: Memory Usage: 193.67578125, Training Duration: 23.748225628\nINFO:root:0: Memory Usage: 192.28125, Training Duration: 24.821648006000032\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "loss", "rule_value": 5, "rule": "loss 5%", "stdout": "", "stderr": "[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,10]<stderr>:INFO:root:10: Memory Usage: 194.02734375, Training Duration: 92.85049901499951\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,3]<stderr>:INFO:root:3: Memory Usage: 192.9453125, Training Duration: 92.71114051199947\n[1,4]<stderr>:INFO:root:4: Memory Usage: 191.921875, Training Duration: 92.71154373399986\n[1,7]<stderr>:INFO:root:7: Memory Usage: 193.6796875, Training Duration: 92.72089324999979\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,9]<stderr>:INFO:root:9: Memory Usage: 191.5234375, Training Duration: 92.91028724900025\n[1,1]<stderr>:INFO:root:1: Memory Usage: 192.52734375, Training Duration: 92.93491952500062\n[1,11]<stderr>:INFO:root:11: Memory Usage: 194.234375, Training Duration: 92.73255053000048\n[1,0]<stderr>:INFO:root:0: Memory Usage: 194.96484375, Training Duration: 92.95683706600084\n[1,8]<stderr>:INFO:root:8: Memory Usage: 191.98828125, Training Duration: 92.75913914600005\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,6]<stderr>:INFO:root:6: Memory Usage: 194.390625, Training Duration: 92.92024883199974\n[1,2]<stderr>:INFO:root:2: Memory Usage: 192.46875, Training Duration: 92.94721668399961\n[1,5]<stderr>:INFO:root:5: Memory Usage: 194.55859375, Training Duration: 93.16833506300009\n", "command": "~/susml/jakob_torben/bin/horovodrun -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "loss", "rule_value": 10, "rule": "loss 10%", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:8: Memory Usage: 191.3828125, Training Duration: 27.41601818000072\nINFO:root:9: Memory Usage: 192.0, Training Duration: 30.317310019000615\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:7: Memory Usage: 194.56640625, Training Duration: 27.416659214999527\nINFO:root:0: Memory Usage: 191.75, Training Duration: 30.58625416399991\nINFO:root:11: Memory Usage: 194.50390625, Training Duration: 30.53928318999988\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:10: Memory Usage: 191.5625, Training Duration: 30.57333347199983\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:5: Memory Usage: 191.8046875, Training Duration: 30.86500299599993\nINFO:root:2: Memory Usage: 192.02734375, Training Duration: 30.885080019999805\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:3: Memory Usage: 192.078125, Training Duration: 28.016134899999997\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:1: Memory Usage: 193.73828125, Training Duration: 30.665841664000254\nINFO:root:6: Memory Usage: 191.84765625, Training Duration: 30.712617176000094\nINFO:root:4: Memory Usage: 193.83984375, Training Duration: 27.997227679999924\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "loss", "rule_value": 10, "rule": "loss 10%", "stdout": "", "stderr": "[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,7]<stderr>:INFO:root:7: Memory Usage: 191.9453125, Training Duration: 98.57061323899961\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,4]<stderr>:INFO:root:4: Memory Usage: 192.32421875, Training Duration: 99.00218766200032\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.34375, Training Duration: 99.18103616999997\n[1,6]<stderr>:INFO:root:6: Memory Usage: 191.234375, Training Duration: 99.18058098500023\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,8]<stderr>:INFO:root:8: Memory Usage: 191.65234375, Training Duration: 99.34265282599972\n[1,10]<stderr>:INFO:root:10: Memory Usage: 191.8046875, Training Duration: 99.3599371249993\n[1,9]<stderr>:INFO:root:9: Memory Usage: 193.11328125, Training Duration: 99.36282611299976\n[1,3]<stderr>:INFO:root:3: Memory Usage: 191.73046875, Training Duration: 99.62243451800077\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,11]<stderr>:INFO:root:11: Memory Usage: 193.09375, Training Duration: 99.61769340299998\n[1,1]<stderr>:INFO:root:1: Memory Usage: 192.140625, Training Duration: 99.85187410100025\n[1,0]<stderr>:INFO:root:0: Memory Usage: 193.984375, Training Duration: 99.89848816699941\n[1,2]<stderr>:INFO:root:2: Memory Usage: 193.2265625, Training Duration: 99.83705551500043\n", "command": "~/susml/jakob_torben/bin/horovodrun -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "loss", "rule_value": 15, "rule": "loss 15%", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:5: Memory Usage: 191.4453125, Training Duration: 404.67339618200003\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:4: Memory Usage: 192.328125, Training Duration: 405.41667056299957\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:6: Memory Usage: 191.31640625, Training Duration: 404.3667467999994\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:2: Memory Usage: 191.97265625, Training Duration: 406.8146229819995\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:3: Memory Usage: 192.171875, Training Duration: 407.5984829879999\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:7: Memory Usage: 193.26171875, Training Duration: 410.94088473400006\nINFO:root:8: Memory Usage: 192.3203125, Training Duration: 411.2241603369994\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:9: Memory Usage: 191.9453125, Training Duration: 280.44737641100073\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:0: Memory Usage: 192.86328125, Training Duration: 559.295519286\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:1: Memory Usage: 192.453125, Training Duration: 411.33446548699976\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:11: Memory Usage: 191.2265625, Training Duration: 411.8931574090002\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:10: Memory Usage: 193.53515625, Training Duration: 281.8289628700004\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "distributed", "rule_type": "delay", "rule_value": 200, "rule": "delay 200ms", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:2: Memory Usage: 193.703125, Training Duration: 46.08744700399984\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:11: Memory Usage: 193.4140625, Training Duration: 45.57673862499996\nINFO:root:0: Memory Usage: 193.50390625, Training Duration: 47.9983351689998\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:1: Memory Usage: 192.859375, Training Duration: 46.03739340100037\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:3: Memory Usage: 192.1640625, Training Duration: 45.06901005199961\nINFO:root:5: Memory Usage: 191.921875, Training Duration: 45.269022156000574\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:6: Memory Usage: 192.53515625, Training Duration: 45.47177058100078\nINFO:root:4: Memory Usage: 192.2734375, Training Duration: 45.472965429000396\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:9: Memory Usage: 193.9765625, Training Duration: 44.669457749000685\nINFO:root:7: Memory Usage: 193.95703125, Training Duration: 44.48769079700014\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:10: Memory Usage: 193.92578125, Training Duration: 44.87206151800001\nINFO:root:8: Memory Usage: 193.54296875, Training Duration: 44.887043853999785\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "delay", "rule_value": 200, "rule": "delay 200ms", "stdout": "", "stderr": "[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792811\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,5]<stderr>:INFO:root:5: Memory Usage: 193.77734375, Training Duration: 128.1371045779997\n[1,3]<stderr>:INFO:root:3: Memory Usage: 193.921875, Training Duration: 127.9533430800002\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,11]<stderr>:INFO:root:11: Memory Usage: 192.5234375, Training Duration: 128.18546333799986\n[1,1]<stderr>:INFO:root:1: Memory Usage: 191.03125, Training Duration: 128.40336368099997\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.453125, Training Duration: 130.20572895999976\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,10]<stderr>:INFO:root:10: Memory Usage: 193.15234375, Training Duration: 127.85667518100036\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,6]<stderr>:INFO:root:6: Memory Usage: 193.60546875, Training Duration: 128.28044660500018\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.70703125, Training Duration: 128.11131500100055\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,4]<stderr>:INFO:root:4: Memory Usage: 192.00390625, Training Duration: 128.33820048400048\n[1,9]<stderr>:INFO:root:9: Memory Usage: 193.0078125, Training Duration: 127.93983150900021\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,2]<stderr>:INFO:root:2: Memory Usage: 193.50390625, Training Duration: 128.5763414510002\n[1,7]<stderr>:INFO:root:7: Memory Usage: 192.49609375, Training Duration: 128.00381145099982\n", "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "delay", "rule_value": 300, "rule": "delay 300ms", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:3: Memory Usage: 192.4140625, Training Duration: 55.8726925990004\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:1: Memory Usage: 193.2109375, Training Duration: 57.39222805099962\nINFO:root:2: Memory Usage: 193.8828125, Training Duration: 57.71550785499949\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:6: Memory Usage: 191.33984375, Training Duration: 56.475327398000445\nINFO:root:4: Memory Usage: 190.9453125, Training Duration: 56.47659561900036\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:0: Memory Usage: 191.6015625, Training Duration: 60.766266405999886\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:8: Memory Usage: 193.31640625, Training Duration: 55.18044120099967\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:7: Memory Usage: 192.55859375, Training Duration: 54.97035473300002\nINFO:root:5: Memory Usage: 194.44140625, Training Duration: 56.771655282999745\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:10: Memory Usage: 192.20703125, Training Duration: 55.34584352999991\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:9: Memory Usage: 192.54296875, Training Duration: 55.4618036920001\nINFO:root:11: Memory Usage: 190.28515625, Training Duration: 57.566694504000225\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "delay", "rule_value": 300, "rule": "delay 300ms", "stdout": "", "stderr": "[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792811\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.64453125, Training Duration: 151.46605199399983\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,3]<stderr>:INFO:root:3: Memory Usage: 193.41015625, Training Duration: 148.18566855599965\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.47265625, Training Duration: 148.4838928980007\n[1,11]<stderr>:INFO:root:11: Memory Usage: 192.16796875, Training Duration: 148.48447445800048\n[1,1]<stderr>:INFO:root:1: Memory Usage: 193.01953125, Training Duration: 148.81012876299974\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,10]<stderr>:INFO:root:10: Memory Usage: 193.71484375, Training Duration: 148.14232262900077\n[1,9]<stderr>:INFO:root:9: Memory Usage: 192.18359375, Training Duration: 148.1488377299993\n[1,4]<stderr>:INFO:root:4: Memory Usage: 192.2890625, Training Duration: 148.7533399089998\n[1,7]<stderr>:INFO:root:7: Memory Usage: 191.71875, Training Duration: 148.16668328500054\n[1,6]<stderr>:INFO:root:6: Memory Usage: 193.875, Training Duration: 148.76851377100047\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,8]<stderr>:INFO:root:8: Memory Usage: 191.953125, Training Duration: 148.50810796899987\n[1,2]<stderr>:INFO:root:2: Memory Usage: 192.87109375, Training Duration: 149.11190867200003\n", "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}, {"trainer": "distributed", "rule_type": "delay", "rule_value": 400, "rule": "delay 400ms", "stdout": "", "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:0: Memory Usage: 192.79296875, Training Duration: 76.4567687660001\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:11: Memory Usage: 192.703125, Training Duration: 70.16119426199998\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:9: Memory Usage: 193.74609375, Training Duration: 67.1528510569999\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:2: Memory Usage: 190.1484375, Training Duration: 70.855462289\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:10: Memory Usage: 189.796875, Training Duration: 67.02316969000003\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:8: Memory Usage: 191.82421875, Training Duration: 67.83637086400006\nINFO:root:6: Memory Usage: 192.44921875, Training Duration: 68.24302156600004\nINFO:root:4: Memory Usage: 190.55859375, Training Duration: 68.64793564800004\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:3: Memory Usage: 191.55859375, Training Duration: 69.23697296299997\nINFO:root:1: Memory Usage: 190.34375, Training Duration: 71.65831036999998\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:7: Memory Usage: 192.640625, Training Duration: 68.64378812999996\nINFO:root:5: Memory Usage: 192.87890625, Training Duration: 69.05135356100004\n", "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"}, {"trainer": "horovod", "rule_type": "delay", "rule_value": 400, "rule": "delay 400ms", "stdout": "", "stderr": "[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792811\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.390625, Training Duration: 179.29253021599993\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,3]<stderr>:INFO:root:3: Memory Usage: 193.3046875, Training Duration: 179.30674587700014\n[1,11]<stderr>:INFO:root:11: Memory Usage: 192.87109375, Training Duration: 179.30911763400013\n[1,10]<stderr>:INFO:root:10: Memory Usage: 193.234375, Training Duration: 178.91333805\n[1,6]<stderr>:INFO:root:6: Memory Usage: 192.1015625, Training Duration: 179.31437437\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.2734375, Training Duration: 183.33439157899988\n[1,1]<stderr>:INFO:root:1: Memory Usage: 191.921875, Training Duration: 179.7337348860001\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,4]<stderr>:INFO:root:4: Memory Usage: 191.78515625, Training Duration: 179.6930714120001\n[1,9]<stderr>:INFO:root:9: Memory Usage: 191.23046875, Training Duration: 179.29437351599995\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,7]<stderr>:INFO:root:7: Memory Usage: 192.875, Training Duration: 179.3072206689999\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.47265625, Training Duration: 179.30618324299985\n[1,2]<stderr>:INFO:root:2: Memory Usage: 192.3203125, Training Duration: 180.13109234800004\n", "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"}]}