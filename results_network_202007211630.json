{
  "results": [
    {
      "trainer": "distributed",
      "rule_type": "delay",
      "rule_value": 0,
      "rule": "delay 0ms",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:3: Memory Usage: 193.4921875, Training Duration: 23.853437240999938\nINFO:root:5: Memory Usage: 193.90625, Training Duration: 23.85758273500005\nINFO:root:4: Memory Usage: 191.30859375, Training Duration: 23.85532500399995\nINFO:root:6: Memory Usage: 193.80859375, Training Duration: 23.876068556999826\nINFO:root:10: Memory Usage: 193.25390625, Training Duration: 23.87231108800006\nINFO:root:7: Memory Usage: 192.94921875, Training Duration: 23.87755223299996\nINFO:root:2: Memory Usage: 192.82421875, Training Duration: 23.875422125999876\nINFO:root:0: Memory Usage: 193.59375, Training Duration: 23.884332821000044\nINFO:root:11: Memory Usage: 191.76953125, Training Duration: 23.88029626299999\nINFO:root:1: Memory Usage: 192.0, Training Duration: 23.875928023999904\nINFO:root:8: Memory Usage: 192.4921875, Training Duration: 23.877040045000058\nINFO:root:9: Memory Usage: 192.3671875, Training Duration: 23.873958162000008\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "delay",
      "rule_value": 0,
      "rule": "delay 0ms",
      "stdout": "",
      "stderr": "[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,4]<stderr>:INFO:root:4: Memory Usage: 193.0859375, Training Duration: 50.316018976999885\n[1,3]<stderr>:INFO:root:3: Memory Usage: 192.74609375, Training Duration: 50.31656792700005\n[1,9]<stderr>:INFO:root:9: Memory Usage: 190.953125, Training Duration: 50.31531238799994\n[1,10]<stderr>:INFO:root:10: Memory Usage: 193.90234375, Training Duration: 50.317166984000096\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.18359375, Training Duration: 50.319329171999925\n[1,11]<stderr>:INFO:root:11: Memory Usage: 191.80078125, Training Duration: 50.316553627000076\n[1,8]<stderr>:INFO:root:8: Memory Usage: 193.046875, Training Duration: 50.31785961199989\n[1,6]<stderr>:INFO:root:6: Memory Usage: 192.125, Training Duration: 50.31654576999995\n[1,2]<stderr>:INFO:root:2: Memory Usage: 192.93359375, Training Duration: 50.31566200900011\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.18359375, Training Duration: 50.31860948799999\n[1,1]<stderr>:INFO:root:1: Memory Usage: 191.58984375, Training Duration: 50.31811770099989\n[1,7]<stderr>:INFO:root:7: Memory Usage: 192.24609375, Training Duration: 50.32612369399999\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "delay",
      "rule_value": 1,
      "rule": "delay 1ms",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:3: Memory Usage: 191.8359375, Training Duration: 25.165148918999876\nINFO:root:6: Memory Usage: 192.61328125, Training Duration: 25.171873141000106\nINFO:root:7: Memory Usage: 194.03125, Training Duration: 25.17746401599993\nINFO:root:8: Memory Usage: 191.52734375, Training Duration: 25.175760024000056\nINFO:root:10: Memory Usage: 191.8125, Training Duration: 25.17319610000004\nINFO:root:9: Memory Usage: 192.8125, Training Duration: 25.169468320000078\nINFO:root:0: Memory Usage: 192.921875, Training Duration: 25.198076722999986\nINFO:root:1: Memory Usage: 192.4140625, Training Duration: 25.183594298000116\nINFO:root:2: Memory Usage: 194.12890625, Training Duration: 25.188202276999846\nINFO:root:5: Memory Usage: 192.79296875, Training Duration: 25.18094003200008\nINFO:root:4: Memory Usage: 191.94140625, Training Duration: 25.183932365000146\nINFO:root:11: Memory Usage: 194.453125, Training Duration: 25.20060085199998\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "delay",
      "rule_value": 1,
      "rule": "delay 1ms",
      "stdout": "",
      "stderr": "[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.96484375, Training Duration: 61.236372770000116\n[1,4]<stderr>:INFO:root:4: Memory Usage: 194.23046875, Training Duration: 61.236864534999995\n[1,3]<stderr>:INFO:root:3: Memory Usage: 193.234375, Training Duration: 61.23869825099996\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.74609375, Training Duration: 61.25821004700015\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.91796875, Training Duration: 61.272726659\n[1,10]<stderr>:INFO:root:10: Memory Usage: 192.640625, Training Duration: 61.259653027999775\n[1,1]<stderr>:INFO:root:1: Memory Usage: 194.17578125, Training Duration: 61.26215929399973\n[1,6]<stderr>:INFO:root:6: Memory Usage: 190.55078125, Training Duration: 61.26147459500021\n[1,2]<stderr>:INFO:root:2: Memory Usage: 192.9921875, Training Duration: 61.27144324400001\n[1,9]<stderr>:INFO:root:9: Memory Usage: 193.22265625, Training Duration: 61.27735181800017\n[1,11]<stderr>:INFO:root:11: Memory Usage: 192.1640625, Training Duration: 61.281877958999985\n[1,7]<stderr>:INFO:root:7: Memory Usage: 193.23828125, Training Duration: 61.28018441600011\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "delay",
      "rule_value": 2,
      "rule": "delay 2ms",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:9: Memory Usage: 192.70703125, Training Duration: 22.750192551000055\nINFO:root:10: Memory Usage: 192.25390625, Training Duration: 22.75257628199961\nINFO:root:11: Memory Usage: 190.96875, Training Duration: 22.77873647100023\nINFO:root:2: Memory Usage: 192.1875, Training Duration: 22.784804612000244\nINFO:root:3: Memory Usage: 190.62890625, Training Duration: 22.781751014000292\nINFO:root:5: Memory Usage: 193.80078125, Training Duration: 22.77668209300009\nINFO:root:4: Memory Usage: 193.07421875, Training Duration: 22.777021618000163\nINFO:root:8: Memory Usage: 193.83203125, Training Duration: 22.77368813000021\nINFO:root:6: Memory Usage: 191.46875, Training Duration: 22.77942344100029\nINFO:root:7: Memory Usage: 194.27734375, Training Duration: 22.780276549000064\nINFO:root:0: Memory Usage: 192.0546875, Training Duration: 22.815195222000057\nINFO:root:1: Memory Usage: 192.671875, Training Duration: 22.79448134999984\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "delay",
      "rule_value": 2,
      "rule": "delay 2ms",
      "stdout": "",
      "stderr": "[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,7]<stderr>:INFO:root:7: Memory Usage: 198.8359375, Training Duration: 74.2195046249999\n[1,1]<stderr>:INFO:root:1: Memory Usage: 198.83203125, Training Duration: 74.22651246199985\n[1,11]<stderr>:INFO:root:11: Memory Usage: 194.7421875, Training Duration: 74.22611525899993\n[1,3]<stderr>:INFO:root:3: Memory Usage: 194.25390625, Training Duration: 74.22479524000028\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.57421875, Training Duration: 74.22397620399988\n[1,9]<stderr>:INFO:root:9: Memory Usage: 193.9296875, Training Duration: 74.22442244900003\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.41796875, Training Duration: 74.22647828699974\n[1,10]<stderr>:INFO:root:10: Memory Usage: 193.55078125, Training Duration: 74.22755694500029\n[1,4]<stderr>:INFO:root:4: Memory Usage: 192.29296875, Training Duration: 74.22885143399981\n[1,2]<stderr>:INFO:root:2: Memory Usage: 191.41796875, Training Duration: 74.25250584600008\n[1,6]<stderr>:INFO:root:6: Memory Usage: 192.51171875, Training Duration: 74.25108117599984\n[1,0]<stderr>:INFO:root:0: Memory Usage: 194.1796875, Training Duration: 74.27395345900004\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "delay",
      "rule_value": 5,
      "rule": "delay 5ms",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:4: Memory Usage: 193.69921875, Training Duration: 22.797911913000007\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:3: Memory Usage: 191.93359375, Training Duration: 22.807403452000017\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:0: Memory Usage: 193.609375, Training Duration: 22.875342104000083\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:10: Memory Usage: 192.34375, Training Duration: 22.793697038999653\nINFO:root:11: Memory Usage: 192.61328125, Training Duration: 22.83396211499985\nINFO:root:1: Memory Usage: 192.69921875, Training Duration: 22.832561020999947\nINFO:root:2: Memory Usage: 191.19140625, Training Duration: 22.84076483800027\nINFO:root:5: Memory Usage: 192.2578125, Training Duration: 22.819631689999824\nINFO:root:6: Memory Usage: 193.47265625, Training Duration: 22.830023320000237\nINFO:root:8: Memory Usage: 192.07421875, Training Duration: 22.81039487199996\nINFO:root:7: Memory Usage: 193.2421875, Training Duration: 22.82146731900002\nINFO:root:9: Memory Usage: 192.40234375, Training Duration: 22.81540136500007\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "delay",
      "rule_value": 5,
      "rule": "delay 5ms",
      "stdout": "",
      "stderr": "[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,7]<stderr>:INFO:root:7: Memory Usage: 191.8984375, Training Duration: 86.23230567300016\n[1,6]<stderr>:INFO:root:6: Memory Usage: 191.1015625, Training Duration: 86.24441711500003\n[1,1]<stderr>:INFO:root:1: Memory Usage: 194.6171875, Training Duration: 86.26330600100027\n[1,3]<stderr>:INFO:root:3: Memory Usage: 193.3515625, Training Duration: 86.25933088500005\n[1,5]<stderr>:INFO:root:5: Memory Usage: 191.9296875, Training Duration: 86.26101642100002\n[1,9]<stderr>:INFO:root:9: Memory Usage: 192.67578125, Training Duration: 86.2571563890001\n[1,8]<stderr>:INFO:root:8: Memory Usage: 193.328125, Training Duration: 86.26081584699978\n[1,10]<stderr>:INFO:root:10: Memory Usage: 192.84765625, Training Duration: 86.2624979430002\n[1,11]<stderr>:INFO:root:11: Memory Usage: 194.24609375, Training Duration: 86.26869153300004\n[1,4]<stderr>:INFO:root:4: Memory Usage: 192.4609375, Training Duration: 86.2682472790002\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.30078125, Training Duration: 86.32272430500007\n[1,2]<stderr>:INFO:root:2: Memory Usage: 193.40625, Training Duration: 86.278359895\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "delay",
      "rule_value": 10,
      "rule": "delay 10ms",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:0: Memory Usage: 193.0078125, Training Duration: 25.644485304000227\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:11: Memory Usage: 192.33203125, Training Duration: 25.567802924999796\nINFO:root:9: Memory Usage: 192.3359375, Training Duration: 25.49475891200018\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:1: Memory Usage: 193.8828125, Training Duration: 25.575493656000162\nINFO:root:10: Memory Usage: 193.73046875, Training Duration: 25.510297363000063\nINFO:root:2: Memory Usage: 193.3515625, Training Duration: 25.588260109000203\nINFO:root:4: Memory Usage: 191.0859375, Training Duration: 25.543280994999805\nINFO:root:7: Memory Usage: 194.72265625, Training Duration: 25.52571690500008\nINFO:root:3: Memory Usage: 194.24609375, Training Duration: 25.561414866000177\nINFO:root:5: Memory Usage: 192.14453125, Training Duration: 25.54708041699996\nINFO:root:6: Memory Usage: 191.69140625, Training Duration: 25.555255254999793\nINFO:root:8: Memory Usage: 193.0703125, Training Duration: 25.52500797599987\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "delay",
      "rule_value": 10,
      "rule": "delay 10ms",
      "stdout": "",
      "stderr": "[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,5]<stderr>:INFO:root:5: Memory Usage: 193.45703125, Training Duration: 89.36297729700027\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,9]<stderr>:INFO:root:9: Memory Usage: 193.09765625, Training Duration: 89.36294738000015\n[1,10]<stderr>:INFO:root:10: Memory Usage: 193.0390625, Training Duration: 89.36454725300018\n[1,1]<stderr>:INFO:root:1: Memory Usage: 192.6484375, Training Duration: 89.38671295299991\n[1,3]<stderr>:INFO:root:3: Memory Usage: 192.7265625, Training Duration: 89.38551600599976\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.02734375, Training Duration: 89.37589231999982\n[1,7]<stderr>:INFO:root:7: Memory Usage: 192.92578125, Training Duration: 89.37716307399978\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.71484375, Training Duration: 89.49324169600004\n[1,2]<stderr>:INFO:root:2: Memory Usage: 192.71484375, Training Duration: 89.40466373000027\n[1,6]<stderr>:INFO:root:6: Memory Usage: 192.81640625, Training Duration: 89.39596778799978\n[1,11]<stderr>:INFO:root:11: Memory Usage: 193.06640625, Training Duration: 89.39910109900029\n[1,4]<stderr>:INFO:root:4: Memory Usage: 190.8671875, Training Duration: 89.398158728\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "delay",
      "rule_value": 25,
      "rule": "delay 25ms",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:5: Memory Usage: 192.78125, Training Duration: 25.53029260099993\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:6: Memory Usage: 192.828125, Training Duration: 25.55674561800015\nINFO:root:4: Memory Usage: 192.328125, Training Duration: 25.558118513999943\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:7: Memory Usage: 193.40625, Training Duration: 25.533802121999997\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:8: Memory Usage: 193.59765625, Training Duration: 25.52977801199995\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:10: Memory Usage: 192.58203125, Training Duration: 25.53270511999972\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:9: Memory Usage: 192.07421875, Training Duration: 25.555541995999647\nINFO:root:11: Memory Usage: 192.07421875, Training Duration: 25.737194005999754\nINFO:root:0: Memory Usage: 193.23046875, Training Duration: 25.982749013999637\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:2: Memory Usage: 192.63671875, Training Duration: 25.783141839999644\nINFO:root:3: Memory Usage: 194.29296875, Training Duration: 25.691248070000256\nINFO:root:1: Memory Usage: 193.2578125, Training Duration: 25.803487790999952\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "delay",
      "rule_value": 25,
      "rule": "delay 25ms",
      "stdout": "",
      "stderr": "[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.828125, Training Duration: 90.48314944000003\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,4]<stderr>:INFO:root:4: Memory Usage: 192.18359375, Training Duration: 90.50931089899996\n[1,10]<stderr>:INFO:root:10: Memory Usage: 192.71875, Training Duration: 90.48467905500002\n[1,3]<stderr>:INFO:root:3: Memory Usage: 192.08203125, Training Duration: 90.50995879999982\n[1,11]<stderr>:INFO:root:11: Memory Usage: 190.90234375, Training Duration: 90.51066847099992\n[1,7]<stderr>:INFO:root:7: Memory Usage: 192.9609375, Training Duration: 90.48469804800015\n[1,6]<stderr>:INFO:root:6: Memory Usage: 191.30078125, Training Duration: 90.51079710900012\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.8515625, Training Duration: 90.78559231600002\n[1,1]<stderr>:INFO:root:1: Memory Usage: 190.75, Training Duration: 90.55898172800016\n[1,9]<stderr>:INFO:root:9: Memory Usage: 193.5859375, Training Duration: 90.50945669700013\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.07421875, Training Duration: 90.53414277499996\n[1,2]<stderr>:INFO:root:2: Memory Usage: 193.33203125, Training Duration: 90.58538305999991\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "delay",
      "rule_value": 50,
      "rule": "delay 50ms",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:5: Memory Usage: 193.69140625, Training Duration: 28.088841056000092\nINFO:root:3: Memory Usage: 192.7890625, Training Duration: 28.141174230999695\nINFO:root:0: Memory Usage: 192.85546875, Training Duration: 28.796638344000257\nINFO:root:11: Memory Usage: 191.3203125, Training Duration: 28.30050692900022\nINFO:root:1: Memory Usage: 191.67578125, Training Duration: 28.34679607499993\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:6: Memory Usage: 193.640625, Training Duration: 28.14127020399974\nINFO:root:4: Memory Usage: 193.81640625, Training Duration: 28.142106081000293\nINFO:root:2: Memory Usage: 193.58203125, Training Duration: 28.39742491900006\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:7: Memory Usage: 193.19921875, Training Duration: 28.092240789999778\nINFO:root:9: Memory Usage: 190.83984375, Training Duration: 28.03754968300018\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:10: Memory Usage: 191.87890625, Training Duration: 28.092346022999664\nINFO:root:8: Memory Usage: 193.08203125, Training Duration: 28.089540973000112\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "delay",
      "rule_value": 50,
      "rule": "delay 50ms",
      "stdout": "",
      "stderr": "[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,3]<stderr>:INFO:root:3: Memory Usage: 193.00390625, Training Duration: 101.55245075700032\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.1171875, Training Duration: 101.58192691100021\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,9]<stderr>:INFO:root:9: Memory Usage: 192.59375, Training Duration: 101.55340204200002\n[1,7]<stderr>:INFO:root:7: Memory Usage: 192.3515625, Training Duration: 101.55569736899997\n[1,1]<stderr>:INFO:root:1: Memory Usage: 193.05078125, Training Duration: 101.66475372200011\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,0]<stderr>:INFO:root:0: Memory Usage: 193.7109375, Training Duration: 102.12000235599999\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,2]<stderr>:INFO:root:2: Memory Usage: 191.3828125, Training Duration: 101.67760525299991\n[1,4]<stderr>:INFO:root:4: Memory Usage: 192.68359375, Training Duration: 101.63365221000004\n[1,10]<stderr>:INFO:root:10: Memory Usage: 191.03125, Training Duration: 101.58369700899993\n[1,11]<stderr>:INFO:root:11: Memory Usage: 194.45703125, Training Duration: 101.63434723799992\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,6]<stderr>:INFO:root:6: Memory Usage: 191.65625, Training Duration: 101.65746876399999\n[1,8]<stderr>:INFO:root:8: Memory Usage: 193.04296875, Training Duration: 101.61828088799984\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "delay",
      "rule_value": 100,
      "rule": "delay 100ms",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:0: Memory Usage: 192.53515625, Training Duration: 35.67950252499986\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:2: Memory Usage: 192.01953125, Training Duration: 34.87979720900012\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:9: Memory Usage: 193.12890625, Training Duration: 34.10104137899998\nINFO:root:11: Memory Usage: 192.69140625, Training Duration: 34.81224541699976\nINFO:root:8: Memory Usage: 192.71875, Training Duration: 34.10946437500024\nINFO:root:10: Memory Usage: 192.51953125, Training Duration: 34.11356671600015\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:3: Memory Usage: 193.01171875, Training Duration: 34.56457440400027\nINFO:root:1: Memory Usage: 193.5390625, Training Duration: 34.977088852999714\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:4: Memory Usage: 191.9453125, Training Duration: 34.56444207000004\nINFO:root:6: Memory Usage: 191.77734375, Training Duration: 34.58240593499977\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:5: Memory Usage: 192.3984375, Training Duration: 34.662792483999965\nINFO:root:7: Memory Usage: 193.76953125, Training Duration: 34.48034290600026\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "delay",
      "rule_value": 100,
      "rule": "delay 100ms",
      "stdout": "",
      "stderr": "[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792811\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,3]<stderr>:INFO:root:3: Memory Usage: 193.6171875, Training Duration: 106.73459236500003\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,5]<stderr>:INFO:root:5: Memory Usage: 194.5703125, Training Duration: 106.74868549200028\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,0]<stderr>:INFO:root:0: Memory Usage: 191.58984375, Training Duration: 107.7824596219998\n[1,1]<stderr>:INFO:root:1: Memory Usage: 191.0, Training Duration: 106.87932253400004\n[1,11]<stderr>:INFO:root:11: Memory Usage: 194.44921875, Training Duration: 106.7808987190001\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,4]<stderr>:INFO:root:4: Memory Usage: 194.1875, Training Duration: 106.82333519899976\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,6]<stderr>:INFO:root:6: Memory Usage: 193.60546875, Training Duration: 106.84400273599977\n[1,10]<stderr>:INFO:root:10: Memory Usage: 194.015625, Training Duration: 106.74832478600001\n[1,9]<stderr>:INFO:root:9: Memory Usage: 193.59375, Training Duration: 106.74495164500013\n[1,2]<stderr>:INFO:root:2: Memory Usage: 193.41015625, Training Duration: 106.95819274299993\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.8125, Training Duration: 106.75985948700009\n[1,7]<stderr>:INFO:root:7: Memory Usage: 194.42578125, Training Duration: 106.77501711800005\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "delay",
      "rule_value": 200,
      "rule": "delay 200ms",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:6: Memory Usage: 191.77734375, Training Duration: 44.69547014400041\nINFO:root:7: Memory Usage: 195.03515625, Training Duration: 44.31185878699989\nINFO:root:8: Memory Usage: 192.31640625, Training Duration: 44.114599473\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:9: Memory Usage: 191.8359375, Training Duration: 44.45612561100006\nINFO:root:0: Memory Usage: 193.4765625, Training Duration: 47.87873447899983\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:10: Memory Usage: 192.140625, Training Duration: 44.49822090999987\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:11: Memory Usage: 190.26171875, Training Duration: 46.10111057699987\nINFO:root:1: Memory Usage: 193.0703125, Training Duration: 46.475771699000234\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:4: Memory Usage: 193.97265625, Training Duration: 45.499039726999854\nINFO:root:2: Memory Usage: 192.109375, Training Duration: 46.517276564999975\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:5: Memory Usage: 192.00390625, Training Duration: 45.85758491700017\nINFO:root:3: Memory Usage: 193.0859375, Training Duration: 45.900204941000084\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "delay",
      "rule_value": 200,
      "rule": "delay 200ms",
      "stdout": "",
      "stderr": "[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792811\tAcc: 17/120 (14%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,5]<stderr>:INFO:root:5: Memory Usage: 193.94140625, Training Duration: 120.73217912700011\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,11]<stderr>:INFO:root:11: Memory Usage: 193.9921875, Training Duration: 120.73246337199998\n[1,3]<stderr>:INFO:root:3: Memory Usage: 192.83203125, Training Duration: 120.74036846199988\n[1,0]<stderr>:INFO:root:0: Memory Usage: 193.140625, Training Duration: 122.75890929900015\n[1,1]<stderr>:INFO:root:1: Memory Usage: 191.94140625, Training Duration: 120.96297302600033\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,6]<stderr>:INFO:root:6: Memory Usage: 193.11328125, Training Duration: 120.82392155100024\n[1,10]<stderr>:INFO:root:10: Memory Usage: 192.7734375, Training Duration: 120.62789681799995\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,9]<stderr>:INFO:root:9: Memory Usage: 193.4453125, Training Duration: 120.69313532900014\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,4]<stderr>:INFO:root:4: Memory Usage: 192.2890625, Training Duration: 120.92789820899998\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,7]<stderr>:INFO:root:7: Memory Usage: 191.96484375, Training Duration: 120.7539330870004\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.1015625, Training Duration: 120.76334894699994\n[1,2]<stderr>:INFO:root:2: Memory Usage: 193.15625, Training Duration: 121.16539357399961\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "delay",
      "rule_value": 300,
      "rule": "delay 300ms",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:0: Memory Usage: 192.16015625, Training Duration: 59.55699687299966\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:11: Memory Usage: 193.61328125, Training Duration: 56.73719954499984\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:5: Memory Usage: 193.79296875, Training Duration: 55.69743873399966\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:3: Memory Usage: 192.41015625, Training Duration: 56.24250324400009\nINFO:root:1: Memory Usage: 192.55859375, Training Duration: 57.457467093000105\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:6: Memory Usage: 192.8203125, Training Duration: 55.99966075999964\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:4: Memory Usage: 193.65625, Training Duration: 56.242409380000026\nINFO:root:2: Memory Usage: 193.65625, Training Duration: 57.76052245700021\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:7: Memory Usage: 193.8359375, Training Duration: 55.715373565999926\nINFO:root:9: Memory Usage: 192.1875, Training Duration: 55.4171857450001\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:10: Memory Usage: 194.03125, Training Duration: 55.70057688799989\nINFO:root:8: Memory Usage: 192.46875, Training Duration: 55.714517865999824\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "delay",
      "rule_value": 300,
      "rule": "delay 300ms",
      "stdout": "",
      "stderr": "[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792811\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782044\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,3]<stderr>:INFO:root:3: Memory Usage: 192.6875, Training Duration: 151.706115339\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,7]<stderr>:INFO:root:7: Memory Usage: 191.8046875, Training Duration: 151.41983468900025\n[1,11]<stderr>:INFO:root:11: Memory Usage: 194.49609375, Training Duration: 151.72101764300032\n[1,5]<stderr>:INFO:root:5: Memory Usage: 193.109375, Training Duration: 151.72303777900015\n[1,0]<stderr>:INFO:root:0: Memory Usage: 193.44921875, Training Duration: 154.74581851499988\n[1,1]<stderr>:INFO:root:1: Memory Usage: 192.3515625, Training Duration: 152.04391399099995\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,9]<stderr>:INFO:root:9: Memory Usage: 191.55859375, Training Duration: 151.7042815220002\n[1,4]<stderr>:INFO:root:4: Memory Usage: 193.51953125, Training Duration: 152.00669982399995\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,10]<stderr>:INFO:root:10: Memory Usage: 193.9296875, Training Duration: 151.72338574199966\n[1,6]<stderr>:INFO:root:6: Memory Usage: 193.203125, Training Duration: 152.02156916400008\n[1,2]<stderr>:INFO:root:2: Memory Usage: 195.1640625, Training Duration: 152.32875651599988\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.7421875, Training Duration: 151.7287827\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "delay",
      "rule_value": 400,
      "rule": "delay 400ms",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:5: Memory Usage: 193.109375, Training Duration: 67.7619635699998\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:4: Memory Usage: 192.078125, Training Duration: 68.04752673599978\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:6: Memory Usage: 193.9296875, Training Duration: 68.16368335800007\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:7: Memory Usage: 193.21875, Training Duration: 67.54610238600026\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:9: Memory Usage: 191.9375, Training Duration: 67.51839069000016\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:11: Memory Usage: 191.37890625, Training Duration: 70.32503045300018\nINFO:root:10: Memory Usage: 193.1328125, Training Duration: 67.52957220899998\nINFO:root:0: Memory Usage: 193.6953125, Training Duration: 74.34299231199975\nINFO:root:8: Memory Usage: 191.71875, Training Duration: 67.54703322400019\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:2: Memory Usage: 190.16015625, Training Duration: 71.14126927500001\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:3: Memory Usage: 193.96875, Training Duration: 69.92475729100033\nINFO:root:1: Memory Usage: 193.5234375, Training Duration: 71.53873645800013\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "delay",
      "rule_value": 400,
      "rule": "delay 400ms",
      "stdout": "",
      "stderr": "[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782044\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,3]<stderr>:INFO:root:3: Memory Usage: 193.4609375, Training Duration: 168.399548719\n[1,5]<stderr>:INFO:root:5: Memory Usage: 193.59765625, Training Duration: 168.40469701100028\n[1,11]<stderr>:INFO:root:11: Memory Usage: 194.78125, Training Duration: 168.4051504289996\n[1,1]<stderr>:INFO:root:1: Memory Usage: 192.703125, Training Duration: 168.8276230270003\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.59375, Training Duration: 172.43689828700053\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,9]<stderr>:INFO:root:9: Memory Usage: 192.57421875, Training Duration: 168.34423615200012\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,4]<stderr>:INFO:root:4: Memory Usage: 192.9609375, Training Duration: 168.79996030300026\n[1,10]<stderr>:INFO:root:10: Memory Usage: 191.75, Training Duration: 168.40354426099975\n[1,6]<stderr>:INFO:root:6: Memory Usage: 190.859375, Training Duration: 168.80368377800005\n[1,7]<stderr>:INFO:root:7: Memory Usage: 193.08984375, Training Duration: 168.40385105800033\n[1,8]<stderr>:INFO:root:8: Memory Usage: 191.66796875, Training Duration: 168.425081029\n[1,2]<stderr>:INFO:root:2: Memory Usage: 193.8046875, Training Duration: 169.2258929270006\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "loss",
      "rule_value": 0,
      "rule": "loss 0%",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:3: Memory Usage: 191.890625, Training Duration: 25.10945082499984\nINFO:root:4: Memory Usage: 193.65625, Training Duration: 25.10726798199994\nINFO:root:5: Memory Usage: 193.59765625, Training Duration: 25.104718258000503\nINFO:root:10: Memory Usage: 192.30859375, Training Duration: 25.12287695499981\nINFO:root:7: Memory Usage: 194.19140625, Training Duration: 25.12883201800014\nINFO:root:6: Memory Usage: 192.19140625, Training Duration: 25.12410205099968\nINFO:root:2: Memory Usage: 191.796875, Training Duration: 25.12477652799953\nINFO:root:1: Memory Usage: 192.1484375, Training Duration: 25.126685617000476\nINFO:root:0: Memory Usage: 193.6484375, Training Duration: 25.135009350000473\nINFO:root:8: Memory Usage: 191.859375, Training Duration: 25.128535531000125\nINFO:root:11: Memory Usage: 191.671875, Training Duration: 25.13290727200001\nINFO:root:9: Memory Usage: 192.1328125, Training Duration: 25.125600526999733\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "loss",
      "rule_value": 0,
      "rule": "loss 0%",
      "stdout": "",
      "stderr": "[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,7]<stderr>:INFO:root:7: Memory Usage: 190.1953125, Training Duration: 49.75244888099951\n[1,11]<stderr>:INFO:root:11: Memory Usage: 192.9140625, Training Duration: 49.75319962499998\n[1,10]<stderr>:INFO:root:10: Memory Usage: 193.91015625, Training Duration: 49.753269298000305\n[1,2]<stderr>:INFO:root:2: Memory Usage: 193.66015625, Training Duration: 49.75434275899988\n[1,9]<stderr>:INFO:root:9: Memory Usage: 192.36328125, Training Duration: 49.75412579800013\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.09375, Training Duration: 49.75427311400017\n[1,1]<stderr>:INFO:root:1: Memory Usage: 192.2109375, Training Duration: 49.75473370799955\n[1,4]<stderr>:INFO:root:4: Memory Usage: 193.15625, Training Duration: 49.75547183500021\n[1,6]<stderr>:INFO:root:6: Memory Usage: 193.9140625, Training Duration: 49.754949653999574\n[1,3]<stderr>:INFO:root:3: Memory Usage: 191.5546875, Training Duration: 49.75637040299989\n[1,0]<stderr>:INFO:root:0: Memory Usage: 194.359375, Training Duration: 49.76785856399965\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.015625, Training Duration: 49.76819650299967\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "loss",
      "rule_value": 0.1,
      "rule": "loss 0.1%",
      "stdout": "",
      "stderr": "[pi01:04127] plm:rsh: Warning: setpgid(4141,4141) failed in parent with errno=Permission denied(13)\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:6: Memory Usage: 191.8984375, Training Duration: 22.447522827000284\nINFO:root:10: Memory Usage: 193.47265625, Training Duration: 22.446854685999824\nINFO:root:4: Memory Usage: 191.5859375, Training Duration: 22.4519828829998\nINFO:root:8: Memory Usage: 193.39453125, Training Duration: 22.452003438999782\nINFO:root:9: Memory Usage: 193.8984375, Training Duration: 22.44657161699979\nINFO:root:3: Memory Usage: 192.609375, Training Duration: 22.455129706000662\nINFO:root:7: Memory Usage: 193.515625, Training Duration: 22.45220305699968\nINFO:root:11: Memory Usage: 192.55078125, Training Duration: 22.45732279599997\nINFO:root:2: Memory Usage: 190.43359375, Training Duration: 22.45316219399956\nINFO:root:5: Memory Usage: 192.5, Training Duration: 22.44878689899997\nINFO:root:1: Memory Usage: 192.1640625, Training Duration: 22.452892336999867\nINFO:root:0: Memory Usage: 191.6796875, Training Duration: 22.461648159000106\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "loss",
      "rule_value": 0.1,
      "rule": "loss 0.1%",
      "stdout": "",
      "stderr": "[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,9]<stderr>:INFO:root:9: Memory Usage: 191.98828125, Training Duration: 54.111155874999895\n[1,10]<stderr>:INFO:root:10: Memory Usage: 194.1015625, Training Duration: 54.11240457400072\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.9375, Training Duration: 54.133017496999855\n[1,6]<stderr>:INFO:root:6: Memory Usage: 194.76953125, Training Duration: 54.133545267000045\n[1,2]<stderr>:INFO:root:2: Memory Usage: 191.859375, Training Duration: 54.135549295999226\n[1,4]<stderr>:INFO:root:4: Memory Usage: 191.9375, Training Duration: 54.133318395999595\n[1,8]<stderr>:INFO:root:8: Memory Usage: 193.48046875, Training Duration: 54.133796875999906\n[1,7]<stderr>:INFO:root:7: Memory Usage: 193.359375, Training Duration: 54.135944916999506\n[1,3]<stderr>:INFO:root:3: Memory Usage: 192.6953125, Training Duration: 54.13570620700011\n[1,1]<stderr>:INFO:root:1: Memory Usage: 192.796875, Training Duration: 54.13858538400018\n[1,11]<stderr>:INFO:root:11: Memory Usage: 195.1796875, Training Duration: 54.13752574299997\n[1,0]<stderr>:INFO:root:0: Memory Usage: 190.484375, Training Duration: 54.14421599100024\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "loss",
      "rule_value": 0.5,
      "rule": "loss 0.5%",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:3: Memory Usage: 192.07421875, Training Duration: 24.348617346000538\nINFO:root:5: Memory Usage: 194.515625, Training Duration: 24.350080076999802\nINFO:root:10: Memory Usage: 192.6640625, Training Duration: 24.345871041000464\nINFO:root:0: Memory Usage: 191.4765625, Training Duration: 24.355746436999652\nINFO:root:7: Memory Usage: 194.5703125, Training Duration: 24.349615932999768\nINFO:root:11: Memory Usage: 190.98828125, Training Duration: 24.3529719750004\nINFO:root:2: Memory Usage: 192.15234375, Training Duration: 24.349935911000102\nINFO:root:6: Memory Usage: 193.09375, Training Duration: 24.348078635000093\nINFO:root:1: Memory Usage: 192.30859375, Training Duration: 24.34970655699999\nINFO:root:8: Memory Usage: 192.08203125, Training Duration: 24.347714395000366\nINFO:root:9: Memory Usage: 191.87109375, Training Duration: 24.353407796999818\nINFO:root:4: Memory Usage: 192.85546875, Training Duration: 24.350970434000374\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "loss",
      "rule_value": 0.5,
      "rule": "loss 0.5%",
      "stdout": "",
      "stderr": "[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,9]<stderr>:INFO:root:9: Memory Usage: 193.36328125, Training Duration: 60.239244559000326\n[1,5]<stderr>:INFO:root:5: Memory Usage: 193.82421875, Training Duration: 60.26116559999991\n[1,0]<stderr>:INFO:root:0: Memory Usage: 193.8203125, Training Duration: 60.26339552799982\n[1,2]<stderr>:INFO:root:2: Memory Usage: 193.78125, Training Duration: 60.26209932799975\n[1,10]<stderr>:INFO:root:10: Memory Usage: 193.02734375, Training Duration: 60.26282815800005\n[1,8]<stderr>:INFO:root:8: Memory Usage: 191.87109375, Training Duration: 60.262754731000314\n[1,4]<stderr>:INFO:root:4: Memory Usage: 193.3359375, Training Duration: 60.26280803300051\n[1,7]<stderr>:INFO:root:7: Memory Usage: 193.390625, Training Duration: 60.26296586999979\n[1,3]<stderr>:INFO:root:3: Memory Usage: 194.81640625, Training Duration: 60.26370214899998\n[1,6]<stderr>:INFO:root:6: Memory Usage: 193.1484375, Training Duration: 60.26421526899958\n[1,11]<stderr>:INFO:root:11: Memory Usage: 192.90234375, Training Duration: 60.26482915699944\n[1,1]<stderr>:INFO:root:1: Memory Usage: 194.55078125, Training Duration: 60.269761176999964\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "loss",
      "rule_value": 1,
      "rule": "loss 1%",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:4: Memory Usage: 192.4765625, Training Duration: 26.339059330000055\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:7: Memory Usage: 194.70703125, Training Duration: 26.35554680400037\nINFO:root:8: Memory Usage: 193.64453125, Training Duration: 26.35377215599965\nINFO:root:11: Memory Usage: 190.69921875, Training Duration: 26.359892655000294\nINFO:root:6: Memory Usage: 192.1328125, Training Duration: 26.356635042000562\nINFO:root:9: Memory Usage: 192.91015625, Training Duration: 26.357939233000252\nINFO:root:0: Memory Usage: 192.28515625, Training Duration: 26.360217894000016\nINFO:root:1: Memory Usage: 191.15625, Training Duration: 26.356151209999553\nINFO:root:2: Memory Usage: 191.98828125, Training Duration: 26.35631780499989\nINFO:root:5: Memory Usage: 193.9453125, Training Duration: 26.358732671000325\nINFO:root:10: Memory Usage: 193.75, Training Duration: 26.36576650400002\nINFO:root:3: Memory Usage: 191.65625, Training Duration: 26.35874177400001\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "loss",
      "rule_value": 1,
      "rule": "loss 1%",
      "stdout": "",
      "stderr": "[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,9]<stderr>:INFO:root:9: Memory Usage: 192.69921875, Training Duration: 70.72608235000007\n[1,10]<stderr>:INFO:root:10: Memory Usage: 191.7578125, Training Duration: 70.72528963900004\n[1,11]<stderr>:INFO:root:11: Memory Usage: 192.81640625, Training Duration: 70.74311372400007\n[1,3]<stderr>:INFO:root:3: Memory Usage: 192.625, Training Duration: 70.74891161899995\n[1,6]<stderr>:INFO:root:6: Memory Usage: 192.61328125, Training Duration: 70.75010841699986\n[1,2]<stderr>:INFO:root:2: Memory Usage: 192.94140625, Training Duration: 70.75063451400001\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.8828125, Training Duration: 70.75314567600071\n[1,1]<stderr>:INFO:root:1: Memory Usage: 192.74609375, Training Duration: 70.75059993200011\n[1,7]<stderr>:INFO:root:7: Memory Usage: 192.7265625, Training Duration: 70.74924160699993\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.12109375, Training Duration: 70.74962832999972\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.59765625, Training Duration: 70.7482479549999\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,4]<stderr>:INFO:root:4: Memory Usage: 193.125, Training Duration: 70.78921180299949\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "loss",
      "rule_value": 2,
      "rule": "loss 2%",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:3: Memory Usage: 191.9921875, Training Duration: 25.893204521999905\nINFO:root:6: Memory Usage: 192.23828125, Training Duration: 25.912064285999804\nINFO:root:9: Memory Usage: 193.28515625, Training Duration: 25.69641588000013\nINFO:root:0: Memory Usage: 193.05078125, Training Duration: 25.91429137399973\nINFO:root:7: Memory Usage: 192.2265625, Training Duration: 25.91350316399985\nINFO:root:11: Memory Usage: 192.07421875, Training Duration: 25.911898156999996\nINFO:root:10: Memory Usage: 191.34375, Training Duration: 25.69690994299981\nINFO:root:8: Memory Usage: 191.5625, Training Duration: 25.910008697000194\nINFO:root:5: Memory Usage: 192.9453125, Training Duration: 25.913156902999617\nINFO:root:4: Memory Usage: 192.09375, Training Duration: 25.915647783000168\nINFO:root:1: Memory Usage: 191.80078125, Training Duration: 25.91001507800047\nINFO:root:2: Memory Usage: 193.65625, Training Duration: 25.910507108000274\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "loss",
      "rule_value": 2,
      "rule": "loss 2%",
      "stdout": "",
      "stderr": "[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,3]<stderr>:INFO:root:3: Memory Usage: 193.9375, Training Duration: 79.80634147799992\n[1,2]<stderr>:INFO:root:2: Memory Usage: 195.0546875, Training Duration: 79.82713545099978\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.2265625, Training Duration: 79.82895080300023\n[1,9]<stderr>:INFO:root:9: Memory Usage: 193.375, Training Duration: 79.75202768100007\n[1,1]<stderr>:INFO:root:1: Memory Usage: 191.72265625, Training Duration: 79.83118164700045\n[1,11]<stderr>:INFO:root:11: Memory Usage: 193.34375, Training Duration: 79.79436104599972\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,0]<stderr>:INFO:root:0: Memory Usage: 193.70703125, Training Duration: 79.84113318399977\n[1,7]<stderr>:INFO:root:7: Memory Usage: 191.72265625, Training Duration: 79.84654146899993\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,10]<stderr>:INFO:root:10: Memory Usage: 192.27734375, Training Duration: 80.03139566600021\n[1,4]<stderr>:INFO:root:4: Memory Usage: 191.6484375, Training Duration: 80.03540208199956\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.21484375, Training Duration: 80.03187066400005\n[1,6]<stderr>:INFO:root:6: Memory Usage: 191.71875, Training Duration: 79.86688910599969\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "loss",
      "rule_value": 5,
      "rule": "loss 5%",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:5: Memory Usage: 191.65234375, Training Duration: 23.538994292000098\nINFO:root:3: Memory Usage: 193.12890625, Training Duration: 24.618579234000208\nINFO:root:4: Memory Usage: 192.96875, Training Duration: 24.621554127999843\nINFO:root:8: Memory Usage: 193.78125, Training Duration: 24.631031505000465\nINFO:root:6: Memory Usage: 193.91015625, Training Duration: 23.559225920000245\nINFO:root:7: Memory Usage: 193.984375, Training Duration: 24.63606492100007\nINFO:root:10: Memory Usage: 192.5859375, Training Duration: 23.56232695000017\nINFO:root:9: Memory Usage: 192.046875, Training Duration: 23.559777616000247\nINFO:root:11: Memory Usage: 192.296875, Training Duration: 24.63737332800065\nINFO:root:0: Memory Usage: 192.75, Training Duration: 24.853580154000156\nINFO:root:1: Memory Usage: 193.96484375, Training Duration: 24.635597591999613\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:2: Memory Usage: 193.02734375, Training Duration: 24.665842849999535\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "loss",
      "rule_value": 5,
      "rule": "loss 5%",
      "stdout": "",
      "stderr": "[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,3]<stderr>:INFO:root:3: Memory Usage: 193.1171875, Training Duration: 90.4366724520005\n[1,2]<stderr>:INFO:root:2: Memory Usage: 194.3828125, Training Duration: 90.45920558299986\n[1,9]<stderr>:INFO:root:9: Memory Usage: 194.46875, Training Duration: 90.25035268099964\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,11]<stderr>:INFO:root:11: Memory Usage: 193.75390625, Training Duration: 90.4690925570003\n[1,7]<stderr>:INFO:root:7: Memory Usage: 190.82421875, Training Duration: 90.28649547699933\n[1,6]<stderr>:INFO:root:6: Memory Usage: 193.9921875, Training Duration: 90.47048720500061\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,5]<stderr>:INFO:root:5: Memory Usage: 192.09375, Training Duration: 90.49785674699979\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.6171875, Training Duration: 90.33919375699952\n[1,10]<stderr>:INFO:root:10: Memory Usage: 192.59765625, Training Duration: 90.34358103500017\n[1,1]<stderr>:INFO:root:1: Memory Usage: 191.9375, Training Duration: 90.55662484899949\n[1,0]<stderr>:INFO:root:0: Memory Usage: 193.078125, Training Duration: 90.5643620789997\n[1,4]<stderr>:INFO:root:4: Memory Usage: 192.921875, Training Duration: 90.57104591200005\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "loss",
      "rule_value": 10,
      "rule": "loss 10%",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:1: Memory Usage: 192.5234375, Training Duration: 29.12057512299998\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:7: Memory Usage: 194.55859375, Training Duration: 28.60461398999996\nINFO:root:8: Memory Usage: 193.78125, Training Duration: 28.60693200700007\nINFO:root:11: Memory Usage: 192.3203125, Training Duration: 29.90051656599917\nINFO:root:10: Memory Usage: 194.140625, Training Duration: 29.684549221999987\nINFO:root:0: Memory Usage: 193.4765625, Training Duration: 30.343140278000647\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:9: Memory Usage: 191.265625, Training Duration: 29.685689007000292\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:3: Memory Usage: 193.81640625, Training Duration: 30.310959746999288\nINFO:root:4: Memory Usage: 193.21484375, Training Duration: 30.305742884999745\nINFO:root:5: Memory Usage: 192.953125, Training Duration: 29.746400508999614\nINFO:root:6: Memory Usage: 192.41796875, Training Duration: 29.766524916999515\nINFO:root:2: Memory Usage: 192.75, Training Duration: 29.761746085000595\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    },
    {
      "trainer": "horovod",
      "rule_type": "loss",
      "rule_value": 10,
      "rule": "loss 10%",
      "stdout": "",
      "stderr": "[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792811\tAcc: 17/120 (14%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,3]<stderr>:INFO:root:3: Memory Usage: 191.94921875, Training Duration: 99.35772308699961\n[1,2]<stderr>:INFO:root:2: Memory Usage: 193.20703125, Training Duration: 99.93478687700008\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,9]<stderr>:INFO:root:9: Memory Usage: 194.46875, Training Duration: 99.66396627799986\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,7]<stderr>:INFO:root:7: Memory Usage: 193.03515625, Training Duration: 99.48148425399995\n[1,11]<stderr>:INFO:root:11: Memory Usage: 194.01953125, Training Duration: 99.48132132699993\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,6]<stderr>:INFO:root:6: Memory Usage: 192.375, Training Duration: 100.03919745999974\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,10]<stderr>:INFO:root:10: Memory Usage: 194.390625, Training Duration: 100.25456063799993\n[1,4]<stderr>:INFO:root:4: Memory Usage: 192.12109375, Training Duration: 99.94992102700053\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.09765625, Training Duration: 100.52950388199952\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.859375, Training Duration: 99.97185632199944\n[1,1]<stderr>:INFO:root:1: Memory Usage: 191.32421875, Training Duration: 97.79996109500007\n[1,5]<stderr>:INFO:root:5: Memory Usage: 193.59375, Training Duration: 100.19366760899993\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "horovod",
      "rule_type": "loss",
      "rule_value": 15,
      "rule": "loss 15%",
      "stdout": "",
      "stderr": "[1,4]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,4]<stderr>:INFO:root:Training set of size 6912\n[1,5]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,5]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,3]<stderr>:INFO:root:Training set of size 6912\n[1,7]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,7]<stderr>:INFO:root:Training set of size 6912\n[1,11]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,11]<stderr>:INFO:root:Training set of size 6912\n[1,6]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,6]<stderr>:INFO:root:Training set of size 6912\n[1,10]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,10]<stderr>:INFO:root:Training set of size 6912\n[1,9]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,9]<stderr>:INFO:root:Training set of size 6912\n[1,8]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,8]<stderr>:INFO:root:Training set of size 6912\n[1,2]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,2]<stderr>:INFO:root:Training set of size 6912\n[1,0]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,0]<stderr>:INFO:root:Training set of size 6912\n[1,1]<stderr>:INFO:root:Preprocessed data found. Skip preprocessing.\n[1,1]<stderr>:INFO:root:Training set of size 6912\n[1,3]<stderr>:INFO:root:Training model for 1 epochs...\n[1,9]<stderr>:INFO:root:Training model for 1 epochs...\n[1,5]<stderr>:INFO:root:Training model for 1 epochs...\n[1,2]<stderr>:INFO:root:Training model for 1 epochs...\n[1,1]<stderr>:INFO:root:Training model for 1 epochs...\n[1,7]<stderr>:INFO:root:Training model for 1 epochs...\n[1,10]<stderr>:INFO:root:Training model for 1 epochs...\n[1,11]<stderr>:INFO:root:Training model for 1 epochs...\n[1,6]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Training model for 1 epochs...\n[1,8]<stderr>:INFO:root:Training model for 1 epochs...\n[1,4]<stderr>:INFO:root:Training model for 1 epochs...\n[1,0]<stderr>:INFO:root:Rank: 00   Start Epoch 0\n[1,1]<stderr>:INFO:root:Rank: 01   Start Epoch 0\n[1,2]<stderr>:INFO:root:Rank: 02   Start Epoch 0\n[1,4]<stderr>:INFO:root:Rank: 04   Start Epoch 0\n[1,3]<stderr>:INFO:root:Rank: 03   Start Epoch 0\n[1,8]<stderr>:INFO:root:Rank: 08   Start Epoch 0\n[1,7]<stderr>:INFO:root:Rank: 07   Start Epoch 0\n[1,11]<stderr>:INFO:root:Rank: 11   Start Epoch 0\n[1,5]<stderr>:INFO:root:Rank: 05   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Start Epoch 0\n[1,6]<stderr>:INFO:root:Rank: 06   Start Epoch 0\n[1,9]<stderr>:INFO:root:Rank: 09   Start Epoch 0\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\n[1,9]<stderr>:INFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\n[1,10]<stderr>:INFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\n[1,7]<stderr>:INFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\n[1,8]<stderr>:INFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\n[1,9]<stderr>:INFO:root:9: Memory Usage: 191.76171875, Training Duration: 127.34762017200046\n[1,10]<stderr>:INFO:root:10: Memory Usage: 191.82421875, Training Duration: 131.99984457400024\n[1,5]<stderr>:INFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\n[1,4]<stderr>:INFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\n[1,5]<stderr>:INFO:root:5: Memory Usage: 193.03125, Training Duration: 132.02124094099963\n[1,4]<stderr>:INFO:root:4: Memory Usage: 193.5625, Training Duration: 132.85021419900022\n[1,7]<stderr>:INFO:root:7: Memory Usage: 192.828125, Training Duration: 132.85061554300046\n[1,8]<stderr>:INFO:root:8: Memory Usage: 192.80078125, Training Duration: 132.85015222400034\n[1,11]<stderr>:INFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\n[1,3]<stderr>:INFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\n[1,0]<stderr>:INFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\n[1,3]<stderr>:INFO:root:3: Memory Usage: 192.01953125, Training Duration: 132.9014133540004\n[1,11]<stderr>:INFO:root:11: Memory Usage: 193.9375, Training Duration: 132.56869861699943\n[1,0]<stderr>:INFO:root:0: Memory Usage: 192.19921875, Training Duration: 133.22366919399974\n[1,2]<stderr>:INFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\n[1,6]<stderr>:INFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\n[1,6]<stderr>:INFO:root:6: Memory Usage: 192.34375, Training Duration: 132.27337539499968\n[1,1]<stderr>:INFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\n[1,1]<stderr>:INFO:root:1: Memory Usage: 192.00390625, Training Duration: 133.26360526500048\n[1,2]<stderr>:INFO:root:2: Memory Usage: 192.80859375, Training Duration: 133.0058536739998\n",
      "command": "~/susml/jakob_torben/bin/horovodrun --start-timeout 300 -np 12 --hosts 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  horovod"
    },
    {
      "trainer": "distributed",
      "rule_type": "loss",
      "rule_value": 15,
      "rule": "loss 15%",
      "stdout": "",
      "stderr": "INFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Preprocessed data found. Skip preprocessing.\nINFO:root:Training set of size 6912\nINFO:root:Training set of size 6912\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 00   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 02   Start Epoch 0\nINFO:root:Rank: 06   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 09   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 10   Start Epoch 0\nINFO:root:Rank: 05   Start Epoch 0\nINFO:root:Rank: 01   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 11   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 03   Start Epoch 0\nINFO:root:Rank: 08   Start Epoch 0\nINFO:root:Training model for 1 epochs...\nINFO:root:Rank: 07   Start Epoch 0\nINFO:root:Rank: 04   Start Epoch 0\nINFO:root:Rank: 03   Train Batch: 1/5 (20%)\tLoss: 1.805652\tAcc: 23/120 (19%)\nINFO:root:Rank: 02   Train Batch: 1/5 (20%)\tLoss: 1.803072\tAcc: 13/120 (11%)\nINFO:root:Rank: 04   Train Batch: 1/5 (20%)\tLoss: 1.787725\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 1/5 (20%)\tLoss: 1.807860\tAcc: 17/120 (14%)\nINFO:root:Rank: 09   Train Batch: 1/5 (20%)\tLoss: 1.795747\tAcc: 24/120 (20%)\nINFO:root:Rank: 00   Train Batch: 1/5 (20%)\tLoss: 1.792827\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 1/5 (20%)\tLoss: 1.798410\tAcc: 18/120 (15%)\nINFO:root:Rank: 10   Train Batch: 1/5 (20%)\tLoss: 1.788200\tAcc: 19/120 (16%)\nINFO:root:Rank: 08   Train Batch: 1/5 (20%)\tLoss: 1.792296\tAcc: 20/120 (17%)\nINFO:root:Rank: 11   Train Batch: 1/5 (20%)\tLoss: 1.794687\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 1/5 (20%)\tLoss: 1.789980\tAcc: 21/120 (18%)\nINFO:root:Rank: 06   Train Batch: 1/5 (20%)\tLoss: 1.780108\tAcc: 26/120 (22%)\nINFO:root:Rank: 00   Train Batch: 2/5 (40%)\tLoss: 1.793929\tAcc: 19/120 (16%)\nINFO:root:Rank: 01   Train Batch: 2/5 (40%)\tLoss: 1.792401\tAcc: 14/120 (12%)\nINFO:root:Rank: 02   Train Batch: 2/5 (40%)\tLoss: 1.809911\tAcc: 10/120 (8%)\nINFO:root:Rank: 11   Train Batch: 2/5 (40%)\tLoss: 1.778764\tAcc: 26/120 (22%)\nINFO:root:Rank: 09   Train Batch: 2/5 (40%)\tLoss: 1.792923\tAcc: 22/120 (18%)\nINFO:root:Rank: 03   Train Batch: 2/5 (40%)\tLoss: 1.791060\tAcc: 23/120 (19%)\nINFO:root:Rank: 05   Train Batch: 2/5 (40%)\tLoss: 1.793685\tAcc: 16/120 (13%)\nINFO:root:Rank: 04   Train Batch: 2/5 (40%)\tLoss: 1.794826\tAcc: 16/120 (13%)\nINFO:root:Rank: 10   Train Batch: 2/5 (40%)\tLoss: 1.790001\tAcc: 20/120 (17%)\nINFO:root:Rank: 08   Train Batch: 2/5 (40%)\tLoss: 1.781578\tAcc: 25/120 (21%)\nINFO:root:Rank: 07   Train Batch: 2/5 (40%)\tLoss: 1.794203\tAcc: 18/120 (15%)\nINFO:root:Rank: 06   Train Batch: 2/5 (40%)\tLoss: 1.787766\tAcc: 22/120 (18%)\nINFO:root:Rank: 00   Train Batch: 3/5 (60%)\tLoss: 1.797802\tAcc: 17/120 (14%)\nINFO:root:Rank: 02   Train Batch: 3/5 (60%)\tLoss: 1.792076\tAcc: 14/120 (12%)\nINFO:root:Rank: 04   Train Batch: 3/5 (60%)\tLoss: 1.789658\tAcc: 22/120 (18%)\nINFO:root:Rank: 05   Train Batch: 3/5 (60%)\tLoss: 1.784656\tAcc: 25/120 (21%)\nINFO:root:Rank: 03   Train Batch: 3/5 (60%)\tLoss: 1.781271\tAcc: 22/120 (18%)\nINFO:root:Rank: 01   Train Batch: 3/5 (60%)\tLoss: 1.777147\tAcc: 25/120 (21%)\nINFO:root:Rank: 11   Train Batch: 3/5 (60%)\tLoss: 1.776799\tAcc: 18/120 (15%)\nINFO:root:Rank: 06   Train Batch: 3/5 (60%)\tLoss: 1.783329\tAcc: 17/120 (14%)\nINFO:root:Rank: 08   Train Batch: 3/5 (60%)\tLoss: 1.776277\tAcc: 21/120 (18%)\nINFO:root:Rank: 07   Train Batch: 3/5 (60%)\tLoss: 1.785756\tAcc: 19/120 (16%)\nINFO:root:Rank: 10   Train Batch: 3/5 (60%)\tLoss: 1.783663\tAcc: 19/120 (16%)\nINFO:root:Rank: 09   Train Batch: 3/5 (60%)\tLoss: 1.792810\tAcc: 17/120 (14%)\nINFO:root:Rank: 02   Train Batch: 4/5 (80%)\tLoss: 1.787267\tAcc: 23/120 (19%)\nINFO:root:Rank: 01   Train Batch: 4/5 (80%)\tLoss: 1.782045\tAcc: 21/120 (18%)\nINFO:root:Rank: 03   Train Batch: 4/5 (80%)\tLoss: 1.783032\tAcc: 20/120 (17%)\nINFO:root:Rank: 04   Train Batch: 4/5 (80%)\tLoss: 1.767982\tAcc: 25/120 (21%)\nINFO:root:Rank: 06   Train Batch: 4/5 (80%)\tLoss: 1.779156\tAcc: 29/120 (24%)\nINFO:root:Rank: 05   Train Batch: 4/5 (80%)\tLoss: 1.779758\tAcc: 22/120 (18%)\nINFO:root:Rank: 07   Train Batch: 4/5 (80%)\tLoss: 1.782339\tAcc: 19/120 (16%)\nINFO:root:Rank: 00   Train Batch: 4/5 (80%)\tLoss: 1.786974\tAcc: 21/120 (18%)\nINFO:root:Rank: 08   Train Batch: 4/5 (80%)\tLoss: 1.777687\tAcc: 21/120 (18%)\nINFO:root:Rank: 11   Train Batch: 4/5 (80%)\tLoss: 1.783836\tAcc: 15/120 (12%)\nINFO:root:Rank: 10   Train Batch: 4/5 (80%)\tLoss: 1.784836\tAcc: 17/120 (14%)\nINFO:root:Rank: 09   Train Batch: 4/5 (80%)\tLoss: 1.774058\tAcc: 26/120 (22%)\nINFO:root:Rank: 02   Train Batch: 5/5 (100%)\tLoss: 1.767738\tAcc: 42/96 (44%)\nINFO:root:2: Memory Usage: 192.4609375, Training Duration: 99.84575108699937\nINFO:root:Rank: 00   Train Batch: 5/5 (100%)\tLoss: 1.782884\tAcc: 32/96 (33%)\nINFO:root:0: Memory Usage: 193.453125, Training Duration: 101.75644829999965\nINFO:root:Rank: 01   Train Batch: 5/5 (100%)\tLoss: 1.781495\tAcc: 32/96 (33%)\nINFO:root:Rank: 07   Train Batch: 5/5 (100%)\tLoss: 1.785762\tAcc: 28/96 (29%)\nINFO:root:1: Memory Usage: 192.0078125, Training Duration: 100.06670659799966\nINFO:root:Rank: 08   Train Batch: 5/5 (100%)\tLoss: 1.777342\tAcc: 27/96 (28%)\nINFO:root:7: Memory Usage: 193.44140625, Training Duration: 98.05708700200012\nINFO:root:8: Memory Usage: 192.046875, Training Duration: 98.11182664599983\nINFO:root:Rank: 09   Train Batch: 5/5 (100%)\tLoss: 1.783321\tAcc: 27/96 (28%)\nINFO:root:9: Memory Usage: 192.3828125, Training Duration: 100.23027453400027\nINFO:root:Rank: 03   Train Batch: 5/5 (100%)\tLoss: 1.776589\tAcc: 31/96 (32%)\nINFO:root:3: Memory Usage: 192.46875, Training Duration: 98.6008556809993\nINFO:root:Rank: 05   Train Batch: 5/5 (100%)\tLoss: 1.768395\tAcc: 41/96 (43%)\nINFO:root:5: Memory Usage: 193.20703125, Training Duration: 100.58094347499991\nINFO:root:Rank: 04   Train Batch: 5/5 (100%)\tLoss: 1.773334\tAcc: 33/96 (34%)\nINFO:root:4: Memory Usage: 192.43359375, Training Duration: 98.63686704700012\nINFO:root:Rank: 06   Train Batch: 5/5 (100%)\tLoss: 1.767290\tAcc: 38/96 (40%)\nINFO:root:6: Memory Usage: 192.26953125, Training Duration: 104.02751202200034\nINFO:root:Rank: 10   Train Batch: 5/5 (100%)\tLoss: 1.788190\tAcc: 29/96 (30%)\nINFO:root:10: Memory Usage: 192.515625, Training Duration: 105.96857873200042\nINFO:root:Rank: 11   Train Batch: 5/5 (100%)\tLoss: 1.772404\tAcc: 37/96 (39%)\nINFO:root:11: Memory Usage: 194.015625, Training Duration: 105.44303826899977\n",
      "command": "mpirun --bind-to none --map-by slot -np 12 --host 10.42.0.50:1,10.42.0.29:1,10.42.0.105:1,10.42.0.56:1,10.42.0.180:1,10.42.0.235:1,10.42.0.244:1,10.42.0.239:1,10.42.0.191:1,10.42.0.41:1,10.42.0.190:1,10.42.0.69:1 ~/susml/jakob_torben/bin/python ~/susml/jakob_torben/src/motion/main.py --batch-size 1440 --epochs 1 --seed 123456789 --no-validation  distributed"
    }
  ]
}